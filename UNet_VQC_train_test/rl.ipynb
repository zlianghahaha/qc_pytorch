{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "controller_params = {\n",
        "    \"sw_space\": ([-1,+1],[-1,+1],[-1,+1],[-1,+1],[-1,+1],[-1,+1],[-1,+1]),\n",
        "    # dataflow 1, dataflow 2, PE for d1, BW for d1\n",
        "    \"hw_space\": (list(range(8,50,8)),list(range(1,9,1)),[32,64],[32,64],[3],[2],[2],[2]),\n",
        "    'max_episodes': 10,\n",
        "    \"num_children_per_episode\": 1,\n",
        "    \"num_hw_per_child\": 10,\n",
        "    'hidden_units': 35,\n",
        "}\n",
        "\n",
        "\n",
        "HW_constraints = {\n",
        "    \"r_Ports_BW\": 1024,\n",
        "    \"r_DSP\": 2520,\n",
        "    \"r_BRAM\": 1824,\n",
        "    \"r_BRAM_Size\": 18000,\n",
        "    \"BITWIDTH\": 16,\n",
        "    \"target_HW_Eff\": 1\n",
        "}"
      ],
      "outputs": [],
      "metadata": {
        "id": "_YuT22AgwLpU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "!pip install termplotlib\n",
        "!pip install tensorflow-cpu==1.15.0\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Collecting termplotlib\n",
            "  Downloading termplotlib-0.3.8-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from termplotlib) (1.19.5)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Installing collected packages: termplotlib\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Successfully installed termplotlib-0.3.8\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQOMNxBUywG_",
        "outputId": "b2cb87a7-61af-4cd6-f6e5-b0cbc7d2da16"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "# %%\n",
        "\n",
        "import logging\n",
        "import csv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "\n",
        "import termplotlib as tpl\n",
        "import copy\n",
        "import random\n",
        "from datetime import datetime\n",
        "import time\n",
        "import torch\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def ema(values):\n",
        "    \"\"\"\n",
        "    Helper function for keeping track of an exponential moving average of a list of values.\n",
        "    For this module, we use it to maintain an exponential moving average of rewards\n",
        "    \"\"\"\n",
        "    weights = np.exp(np.linspace(-1., 0., len(values)))\n",
        "    weights /= weights.sum()\n",
        "    a = np.convolve(values, weights, mode=\"full\")[:len(values)]\n",
        "    return a[-1]\n",
        "\n",
        "\n",
        "class Controller(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.graph = tf.Graph()\n",
        "\n",
        "        config = tf.ConfigProto()\n",
        "        config.gpu_options.allow_growth = True\n",
        "        self.sess = tf.Session(config=config, graph=self.graph)\n",
        "\n",
        "        self.hidden_units = controller_params['hidden_units']\n",
        "\n",
        "        self.nn1_search_space = controller_params['sw_space']\n",
        "        self.hw1_search_space = controller_params['hw_space']\n",
        "\n",
        "        self.nn1_num_para = len(self.nn1_search_space)\n",
        "        self.hw1_num_para = len(self.hw1_search_space)\n",
        "\n",
        "\n",
        "        self.num_para = self.nn1_num_para + self.hw1_num_para\n",
        "\n",
        "        self.nn1_beg, self.nn1_end = 0, self.nn1_num_para\n",
        "        self.hw1_beg, self.hw1_end = self.nn1_end, self.nn1_end + self.hw1_num_para\n",
        "\n",
        "        self.para_2_val = {}\n",
        "        idx = 0\n",
        "        for hp in self.nn1_search_space:\n",
        "            self.para_2_val[idx] = hp\n",
        "            idx += 1\n",
        "        for hp in self.hw1_search_space:\n",
        "            self.para_2_val[idx] = hp\n",
        "            idx += 1\n",
        "\n",
        "\n",
        "        self.RNN_classifier = {}\n",
        "        self.RNN_pred_prob = {}\n",
        "        with self.graph.as_default():\n",
        "            self.build_controller()\n",
        "\n",
        "        self.reward_history = []\n",
        "        self.architecture_history = []\n",
        "        self.trained_network = {}\n",
        "\n",
        "        self.explored_info = {}\n",
        "\n",
        "        self.target_HW_Eff = HW_constraints[\"target_HW_Eff\"]\n",
        "\n",
        "    def build_controller(self):\n",
        "        logger.info('Building RNN Network')\n",
        "        # Build inputs and placeholders\n",
        "        with tf.name_scope('controller_inputs'):\n",
        "            # Input to the NASCell\n",
        "            self.child_network_paras = tf.placeholder(tf.int64, [None, self.num_para], name='controller_input')\n",
        "            # Discounted rewards\n",
        "            self.discounted_rewards = tf.placeholder(tf.float32, (None,), name='discounted_rewards')\n",
        "            # WW 12-18: input: the batch_size variable will be used to determine the RNN batch\n",
        "            self.batch_size = tf.placeholder(tf.int32, [], name='batch_size')\n",
        "\n",
        "        with tf.name_scope('embedding'):\n",
        "            self.embedding_weights = []\n",
        "            # share embedding weights for each type of parameters\n",
        "            embedding_id = 0\n",
        "            para_2_emb_id = {}\n",
        "            for i in range(len(self.para_2_val.keys())):\n",
        "                additional_para_size = len(self.para_2_val[i])\n",
        "                additional_para_weights = tf.get_variable('state_embeddings_%d' % (embedding_id),\n",
        "                                                          shape=[additional_para_size, self.hidden_units],\n",
        "                                                          initializer=tf.initializers.random_uniform(-1., 1.))\n",
        "                self.embedding_weights.append(additional_para_weights)\n",
        "                para_2_emb_id[i] = embedding_id\n",
        "                embedding_id += 1\n",
        "\n",
        "            self.embedded_input_list = []\n",
        "            for i in range(self.num_para):\n",
        "                self.embedded_input_list.append(\n",
        "                    tf.nn.embedding_lookup(self.embedding_weights[para_2_emb_id[i]], self.child_network_paras[:, i]))\n",
        "            self.embedded_input = tf.stack(self.embedded_input_list, axis=-1)\n",
        "            self.embedded_input = tf.transpose(self.embedded_input, perm=[0, 2, 1])\n",
        "\n",
        "        logger.info('Building Controller')\n",
        "        with tf.name_scope('controller'):\n",
        "            with tf.variable_scope('RNN'):\n",
        "                nas = tf.contrib.rnn.NASCell(self.hidden_units)\n",
        "                tmp_state = nas.zero_state(batch_size=self.batch_size, dtype=tf.float32)\n",
        "                init_state = tf.nn.rnn_cell.LSTMStateTuple(tmp_state[0], tmp_state[1])\n",
        "\n",
        "                output, final_state = tf.nn.dynamic_rnn(nas, self.embedded_input, initial_state=init_state,\n",
        "                                                        dtype=tf.float32)\n",
        "                tmp_list = []\n",
        "                # print(\"output\",\"=\"*50,output)\n",
        "                # print(\"output slice\",\"=\"*50,output[:,-1,:])\n",
        "                for para_idx in range(self.num_para):\n",
        "                    o = output[:, para_idx, :]\n",
        "                    para_len = len(self.para_2_val[para_idx])\n",
        "                    # len(self.para_val[para_idx % self.para_per_layer])\n",
        "                    classifier = tf.layers.dense(o, units=para_len, name='classifier_%d' % (para_idx), reuse=False)\n",
        "                    self.RNN_classifier[para_idx] = classifier\n",
        "                    prob_pred = tf.nn.softmax(classifier)\n",
        "                    self.RNN_pred_prob[para_idx] = prob_pred\n",
        "                    child_para = tf.argmax(prob_pred, axis=-1)\n",
        "                    tmp_list.append(child_para)\n",
        "                self.pred_val = tf.stack(tmp_list, axis=1)\n",
        "\n",
        "        logger.info('Building Optimization')\n",
        "        # with tf.name_scope('Optimization'):\n",
        "        # Global Optimization composes all RNNs in one, like NAS, where arch_idx = 0\n",
        "\n",
        "        with tf.name_scope('Optimizer'):\n",
        "            self.global_step = tf.Variable(0, trainable=False)\n",
        "            self.learning_rate = tf.train.exponential_decay(0.99, self.global_step, 50, 0.5, staircase=True)\n",
        "            self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate)\n",
        "        # self.optimizer = tf.train.AdamOptimizer(learning_rate=0.1)\n",
        "        with tf.name_scope('Loss'):\n",
        "            # We seperately compute loss of each predict parameter since the dim of predicting parameters may not be same\n",
        "            for para_idx in range(self.num_para):\n",
        "                if para_idx == 0:\n",
        "                    self.policy_gradient_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                        logits=self.RNN_classifier[para_idx], labels=self.child_network_paras[:, para_idx])\n",
        "                else:\n",
        "                    self.policy_gradient_loss = tf.add(self.policy_gradient_loss,\n",
        "                                                       tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                                                           logits=self.RNN_classifier[para_idx],\n",
        "                                                           labels=self.child_network_paras[:, para_idx]))\n",
        "                # get mean of loss\n",
        "            self.policy_gradient_loss /= self.num_para\n",
        "            self.total_loss = self.policy_gradient_loss\n",
        "            self.gradients = self.optimizer.compute_gradients(self.total_loss)\n",
        "\n",
        "            # Gradients calculated using REINFORCE\n",
        "            for i, (grad, var) in enumerate(self.gradients):\n",
        "                if grad is not None:\n",
        "                    # print(\"aaa\",grad)\n",
        "                    # print(\"aaa\",self.discounted_rewards)\n",
        "                    # sys.exit(0)\n",
        "                    self.gradients[i] = (grad * self.discounted_rewards, var)\n",
        "\n",
        "        with tf.name_scope('Train_RNN'):\n",
        "            # The main training operation. This applies REINFORCE on the weights of the Controller\n",
        "            # self.train_operation[arch_idx][pip_idx] = self.optimizer[arch_idx][pip_idx].apply_gradients(self.gradients[arch_idx][pip_idx], global_step=self.global_step[arch_idx][pip_idx])\n",
        "            # self.train_operation = self.optimizer.minimize(self.total_loss)\n",
        "            self.train_operation = self.optimizer.apply_gradients(self.gradients)\n",
        "            self.update_global_step = tf.assign(self.global_step, self.global_step + 1, name='update_global_step')\n",
        "\n",
        "        logger.info('Successfully built controller')\n",
        "\n",
        "    def child_network_translate(self, child_network):\n",
        "        dnn_out = np.zeros_like(child_network)\n",
        "        for para_idx in range(self.num_para):\n",
        "            dnn_out[0][para_idx] = (self.para_2_val[para_idx][child_network[0][para_idx]])\n",
        "        return dnn_out\n",
        "\n",
        "    def generate_child_network(self, child_network_architecture):\n",
        "        with self.graph.as_default():\n",
        "            feed_dict = {\n",
        "                self.child_network_paras: child_network_architecture,\n",
        "                self.batch_size: 1\n",
        "            }\n",
        "            rnn_out = self.sess.run(self.RNN_pred_prob, feed_dict=feed_dict)\n",
        "            predict_child = np.array([[0] * self.num_para])\n",
        "            # random.seed(datetime.now())\n",
        "            for para_idx, prob in rnn_out.items():\n",
        "                predict_child[0][para_idx] = np.random.choice(range(len(self.para_2_val[para_idx])), p=prob[0])\n",
        "            hyperparameters = self.child_network_translate(predict_child)\n",
        "            return predict_child, hyperparameters\n",
        "\n",
        "    def plot_history(self, history, ylim=(-1, 1), title=\"reward\"):\n",
        "        x = list(range(len(history)))\n",
        "        y = history\n",
        "        fig = tpl.figure()\n",
        "        fig.plot(x, y, ylim=ylim, width=60, height=20, title=title)\n",
        "        fig.show()\n",
        "\n",
        "    def get_HW_efficienct(self, Network, HW1, RC):\n",
        "        # Weiwen 01-24: Using the built Network and HW1 explored results to generate hardware efficiency\n",
        "        # with the consideration of resource constraint RC\n",
        "        return random.uniform(0, 1)\n",
        "\n",
        "    def para2interface_NN(self, Para_NN1):\n",
        "        # Weiwen 01-24: Build NN using explored hyperparamters, return Network\n",
        "        Network = -1    # func(Para_NN1)\n",
        "        return Network\n",
        "\n",
        "    def para2interface_HW(self, Para_HW1):\n",
        "        # Weiwen 01-24: Build hardware model using the explored paramters\n",
        "        HW1 = -1        # func(Para_HW1)\n",
        "        RC = [HW_constraints[\"r_Ports_BW\"],\n",
        "              HW_constraints[\"r_DSP\"],\n",
        "              HW_constraints[\"r_BRAM\"],\n",
        "              HW_constraints[\"r_BRAM_Size\"],\n",
        "              HW_constraints[\"BITWIDTH\"]]\n",
        "        return HW1, RC\n",
        "\n",
        "    def global_train(self):\n",
        "        with self.graph.as_default():\n",
        "            self.sess.run(tf.global_variables_initializer())\n",
        "        step = 0\n",
        "        total_rewards = 0\n",
        "        child_network = np.array([[0] * self.num_para], dtype=np.int64)\n",
        "\n",
        "        for episode in range(controller_params['max_episodes']):\n",
        "            logger.info(\n",
        "                '=-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode {}<=-=-==-=-==-=-==-=-==-=-==-=-==-=-='.format(episode))\n",
        "            step += 1\n",
        "            episode_reward_buffer = []\n",
        "            arachitecture_batch = []\n",
        "\n",
        "            if episode % 50 == 0 and episode != 0:\n",
        "                print(\"Process:\", str(float(episode) / controller_params['max_episodes'] * 100) + \"%\", file=sys.stderr)\n",
        "                # self.plot_history(self.reward_history, ylim=(min(self.reward_history)-0.01, max(self.reward_history)+0.01))\n",
        "\n",
        "            for sub_child in range(controller_params[\"num_children_per_episode\"]):\n",
        "                # Generate a child network architecture\n",
        "                child_network, hyperparameters = self.generate_child_network(child_network)\n",
        "\n",
        "                DNA_NN1 = child_network[0][self.nn1_beg:self.nn1_end]\n",
        "                DNA_HW1 = child_network[0][self.hw1_beg:self.hw1_end]\n",
        "\n",
        "\n",
        "                Para_NN1 = hyperparameters[0][self.nn1_beg:self.nn1_end]\n",
        "                Para_HW1 = hyperparameters[0][self.hw1_beg:self.hw1_end]\n",
        "                \n",
        "\n",
        "                str_NN1 = \" \".join(str(x) for x in Para_NN1)\n",
        "                str_NNs = str_NN1\n",
        "\n",
        "                str_HW1 = \" \".join(str(x) for x in Para_HW1)\n",
        "                str_HWs = str_HW1\n",
        "\n",
        "                logger.info('=====>Step {}/{} in episode {}: HyperParameters: {} <====='.format(sub_child, \\\n",
        "                                                                                                controller_params[\n",
        "                                                                                                    \"num_children_per_episode\"],\n",
        "                                                                                                episode,\n",
        "                                                                                                hyperparameters))\n",
        "\n",
        "                if str_NNs in self.explored_info.keys():\n",
        "                    accuracy = self.explored_info[str_NNs][0]\n",
        "                    reward = self.explored_info[str_NNs][1]\n",
        "                    \n",
        "\n",
        "                else:\n",
        "\n",
        "                    accuracy = self.para2interface_NN(Para_NN1)\n",
        "                    reward = accuracy\n",
        "\n",
        "\n",
        "                logger.info(\"====================Results=======================\")\n",
        "                logger.info(\"--------->NN: {}, Accuracy: {}\".format(str_NNs, accuracy))\n",
        "                logger.info(\"--------->Reward: {}\".format(reward))\n",
        "                logger.info(\"=\" * 50)\n",
        "\n",
        "                episode_reward_buffer.append(reward)\n",
        "                identified_arch = np.array(\n",
        "                    list(DNA_NN1) + list(DNA_HW1))\n",
        "                arachitecture_batch.append(identified_arch)\n",
        "\n",
        "            current_reward = np.array(episode_reward_buffer)\n",
        "\n",
        "            mean_reward = np.mean(current_reward)\n",
        "            self.reward_history.append(mean_reward)\n",
        "            self.architecture_history.append(child_network)\n",
        "            total_rewards += mean_reward\n",
        "\n",
        "            baseline = ema(self.reward_history)\n",
        "            last_reward = self.reward_history[-1]\n",
        "            # rewards = current_reward - baseline\n",
        "            rewards = [last_reward - baseline]\n",
        "\n",
        "            feed_dict = {\n",
        "                self.child_network_paras: arachitecture_batch,\n",
        "                self.batch_size: len(arachitecture_batch),\n",
        "                self.discounted_rewards: rewards\n",
        "            }\n",
        "\n",
        "            with self.graph.as_default():\n",
        "                _, _, loss, lr, gs = self.sess.run(\n",
        "                    [self.train_operation, self.update_global_step, self.total_loss, self.learning_rate,\n",
        "                     self.global_step], feed_dict=feed_dict)\n",
        "\n",
        "            logger.info('=-=-=-=-=-=>Episode: {} | Loss: {} | LR: {} | Mean R: {} | Reward: {}<=-=-=-=-='.format(\n",
        "                episode, loss, (lr, gs), mean_reward, rewards))\n",
        "\n",
        "        print(self.reward_history)\n",
        "        # self.plot_history(self.reward_history, ylim=(min(self.reward_history)-0.01, max(self.reward_history)-0.01))\n",
        "\n",
        "\n",
        "# %%\n",
        "\n",
        "\n",
        "seed = 0\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "logging.basicConfig(stream=sys.stdout,\n",
        "                    level=logging.DEBUG,\n",
        "                    format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s')\n",
        "\n",
        "print(\"Begin\")\n",
        "controller = Controller()\n",
        "controller.global_train()\n",
        "\n",
        "# %%\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin\n",
            "2021-08-17 06:31:58,396 __main__     INFO     Building RNN Network\n",
            "2021-08-17 06:31:58,546 __main__     INFO     Building Controller\n",
            "2021-08-17 06:31:58,887 __main__     INFO     Building Optimization\n",
            "2021-08-17 06:32:00,397 __main__     INFO     Successfully built controller\n",
            "2021-08-17 06:32:00,615 __main__     INFO     =-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 0<=-=-==-=-==-=-==-=-==-=-==-=-==-=-=\n",
            "2021-08-17 06:32:00,724 __main__     INFO     =====>Step 0/1 in episode 0: HyperParameters: [[-1  1  1  1 -1 -1  1 48  7 64 32  3  2  2  2]] <=====\n",
            "2021-08-17 06:32:00,725 __main__     INFO     ====================Results=======================\n",
            "2021-08-17 06:32:00,728 __main__     INFO     --------->NN: -1 1 1 1 -1 -1 1, Accuracy: -1\n",
            "2021-08-17 06:32:00,730 __main__     INFO     --------->Reward: -1\n",
            "2021-08-17 06:32:00,733 __main__     INFO     ==================================================\n",
            "2021-08-17 06:32:01,275 __main__     INFO     =-=-=-=-=-=>Episode: 0 | Loss: [0.6702406] | LR: (0.99, 1) | Mean R: -1.0 | Reward: [0.0]<=-=-=-=-=\n",
            "2021-08-17 06:32:01,276 __main__     INFO     =-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 1<=-=-==-=-==-=-==-=-==-=-==-=-==-=-=\n",
            "2021-08-17 06:32:01,283 __main__     INFO     =====>Step 0/1 in episode 1: HyperParameters: [[-1  1  1 -1  1  1 -1 24  2 64 32  3  2  2  2]] <=====\n",
            "2021-08-17 06:32:01,285 __main__     INFO     ====================Results=======================\n",
            "2021-08-17 06:32:01,286 __main__     INFO     --------->NN: -1 1 1 -1 1 1 -1, Accuracy: -1\n",
            "2021-08-17 06:32:01,287 __main__     INFO     --------->Reward: -1\n",
            "2021-08-17 06:32:01,289 __main__     INFO     ==================================================\n",
            "2021-08-17 06:32:01,305 __main__     INFO     =-=-=-=-=-=>Episode: 1 | Loss: [0.6792029] | LR: (0.99, 2) | Mean R: -1.0 | Reward: [0.0]<=-=-=-=-=\n",
            "2021-08-17 06:32:01,306 __main__     INFO     =-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 2<=-=-==-=-==-=-==-=-==-=-==-=-==-=-=\n",
            "2021-08-17 06:32:01,314 __main__     INFO     =====>Step 0/1 in episode 2: HyperParameters: [[-1 -1  1 -1  1 -1 -1 32  7 32 32  3  2  2  2]] <=====\n",
            "2021-08-17 06:32:01,314 __main__     INFO     ====================Results=======================\n",
            "2021-08-17 06:32:01,318 __main__     INFO     --------->NN: -1 -1 1 -1 1 -1 -1, Accuracy: -1\n",
            "2021-08-17 06:32:01,320 __main__     INFO     --------->Reward: -1\n",
            "2021-08-17 06:32:01,321 __main__     INFO     ==================================================\n",
            "2021-08-17 06:32:01,333 __main__     INFO     =-=-=-=-=-=>Episode: 2 | Loss: [0.677008] | LR: (0.99, 3) | Mean R: -1.0 | Reward: [-1.1102230246251565e-16]<=-=-=-=-=\n",
            "2021-08-17 06:32:01,335 __main__     INFO     =-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 3<=-=-==-=-==-=-==-=-==-=-==-=-==-=-=\n",
            "2021-08-17 06:32:01,342 __main__     INFO     =====>Step 0/1 in episode 3: HyperParameters: [[ 1  1  1  1  1 -1  1 32  6 64 64  3  2  2  2]] <=====\n",
            "2021-08-17 06:32:01,343 __main__     INFO     ====================Results=======================\n",
            "2021-08-17 06:32:01,344 __main__     INFO     --------->NN: 1 1 1 1 1 -1 1, Accuracy: -1\n",
            "2021-08-17 06:32:01,346 __main__     INFO     --------->Reward: -1\n",
            "2021-08-17 06:32:01,347 __main__     INFO     ==================================================\n",
            "2021-08-17 06:32:01,362 __main__     INFO     =-=-=-=-=-=>Episode: 3 | Loss: [0.6802985] | LR: (0.99, 4) | Mean R: -1.0 | Reward: [-1.1102230246251565e-16]<=-=-=-=-=\n",
            "2021-08-17 06:32:01,363 __main__     INFO     =-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 4<=-=-==-=-==-=-==-=-==-=-==-=-==-=-=\n",
            "2021-08-17 06:32:01,369 __main__     INFO     =====>Step 0/1 in episode 4: HyperParameters: [[-1  1 -1 -1  1  1  1 24  2 64 64  3  2  2  2]] <=====\n",
            "2021-08-17 06:32:01,370 __main__     INFO     ====================Results=======================\n",
            "2021-08-17 06:32:01,371 __main__     INFO     --------->NN: -1 1 -1 -1 1 1 1, Accuracy: -1\n",
            "2021-08-17 06:32:01,372 __main__     INFO     --------->Reward: -1\n",
            "2021-08-17 06:32:01,373 __main__     INFO     ==================================================\n",
            "2021-08-17 06:32:01,387 __main__     INFO     =-=-=-=-=-=>Episode: 4 | Loss: [0.6755982] | LR: (0.99, 5) | Mean R: -1.0 | Reward: [0.0]<=-=-=-=-=\n",
            "2021-08-17 06:32:01,388 __main__     INFO     =-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 5<=-=-==-=-==-=-==-=-==-=-==-=-==-=-=\n",
            "2021-08-17 06:32:01,395 __main__     INFO     =====>Step 0/1 in episode 5: HyperParameters: [[-1 -1 -1 -1 -1  1 -1 48  1 32 64  3  2  2  2]] <=====\n",
            "2021-08-17 06:32:01,396 __main__     INFO     ====================Results=======================\n",
            "2021-08-17 06:32:01,397 __main__     INFO     --------->NN: -1 -1 -1 -1 -1 1 -1, Accuracy: -1\n",
            "2021-08-17 06:32:01,398 __main__     INFO     --------->Reward: -1\n",
            "2021-08-17 06:32:01,399 __main__     INFO     ==================================================\n",
            "2021-08-17 06:32:01,410 __main__     INFO     =-=-=-=-=-=>Episode: 5 | Loss: [0.67864394] | LR: (0.99, 6) | Mean R: -1.0 | Reward: [0.0]<=-=-=-=-=\n",
            "2021-08-17 06:32:01,411 __main__     INFO     =-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 6<=-=-==-=-==-=-==-=-==-=-==-=-==-=-=\n",
            "2021-08-17 06:32:01,416 __main__     INFO     =====>Step 0/1 in episode 6: HyperParameters: [[ 1  1 -1  1  1 -1  1 16  4 64 64  3  2  2  2]] <=====\n",
            "2021-08-17 06:32:01,417 __main__     INFO     ====================Results=======================\n",
            "2021-08-17 06:32:01,418 __main__     INFO     --------->NN: 1 1 -1 1 1 -1 1, Accuracy: -1\n",
            "2021-08-17 06:32:01,419 __main__     INFO     --------->Reward: -1\n",
            "2021-08-17 06:32:01,420 __main__     INFO     ==================================================\n",
            "2021-08-17 06:32:01,432 __main__     INFO     =-=-=-=-=-=>Episode: 6 | Loss: [0.67896026] | LR: (0.99, 7) | Mean R: -1.0 | Reward: [-1.1102230246251565e-16]<=-=-=-=-=\n",
            "2021-08-17 06:32:01,433 __main__     INFO     =-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 7<=-=-==-=-==-=-==-=-==-=-==-=-==-=-=\n",
            "2021-08-17 06:32:01,438 __main__     INFO     =====>Step 0/1 in episode 7: HyperParameters: [[-1  1  1 -1  1  1  1 24  4 64 64  3  2  2  2]] <=====\n",
            "2021-08-17 06:32:01,439 __main__     INFO     ====================Results=======================\n",
            "2021-08-17 06:32:01,440 __main__     INFO     --------->NN: -1 1 1 -1 1 1 1, Accuracy: -1\n",
            "2021-08-17 06:32:01,441 __main__     INFO     --------->Reward: -1\n",
            "2021-08-17 06:32:01,442 __main__     INFO     ==================================================\n",
            "2021-08-17 06:32:01,457 __main__     INFO     =-=-=-=-=-=>Episode: 7 | Loss: [0.67615795] | LR: (0.99, 8) | Mean R: -1.0 | Reward: [2.220446049250313e-16]<=-=-=-=-=\n",
            "2021-08-17 06:32:01,459 __main__     INFO     =-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 8<=-=-==-=-==-=-==-=-==-=-==-=-==-=-=\n",
            "2021-08-17 06:32:01,467 __main__     INFO     =====>Step 0/1 in episode 8: HyperParameters: [[-1  1  1  1  1  1  1 24  1 32 32  3  2  2  2]] <=====\n",
            "2021-08-17 06:32:01,468 __main__     INFO     ====================Results=======================\n",
            "2021-08-17 06:32:01,469 __main__     INFO     --------->NN: -1 1 1 1 1 1 1, Accuracy: -1\n",
            "2021-08-17 06:32:01,470 __main__     INFO     --------->Reward: -1\n",
            "2021-08-17 06:32:01,471 __main__     INFO     ==================================================\n",
            "2021-08-17 06:32:01,487 __main__     INFO     =-=-=-=-=-=>Episode: 8 | Loss: [0.6621395] | LR: (0.99, 9) | Mean R: -1.0 | Reward: [-1.1102230246251565e-16]<=-=-=-=-=\n",
            "2021-08-17 06:32:01,488 __main__     INFO     =-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode 9<=-=-==-=-==-=-==-=-==-=-==-=-==-=-=\n",
            "2021-08-17 06:32:01,493 __main__     INFO     =====>Step 0/1 in episode 9: HyperParameters: [[ 1 -1  1 -1  1  1 -1 24  6 32 32  3  2  2  2]] <=====\n",
            "2021-08-17 06:32:01,495 __main__     INFO     ====================Results=======================\n",
            "2021-08-17 06:32:01,496 __main__     INFO     --------->NN: 1 -1 1 -1 1 1 -1, Accuracy: -1\n",
            "2021-08-17 06:32:01,497 __main__     INFO     --------->Reward: -1\n",
            "2021-08-17 06:32:01,498 __main__     INFO     ==================================================\n",
            "2021-08-17 06:32:01,510 __main__     INFO     =-=-=-=-=-=>Episode: 9 | Loss: [0.677954] | LR: (0.99, 10) | Mean R: -1.0 | Reward: [2.220446049250313e-16]<=-=-=-=-=\n",
            "[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUIKNlGDwE6Y",
        "outputId": "13cef2dc-6a8b-4bd3-fd2f-6075b42ecf57"
      }
    }
  ]
}