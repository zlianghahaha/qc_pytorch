{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.6.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.13 64-bit ('py36': conda)"
    },
    "interpreter": {
      "hash": "033d838eb552c49c1dd1955190364a31215ed7c17e10834f4abb1f99a547f6df"
    },
    "colab": {
      "name": "liang.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 47,
      "source": [
        "is_colab = False\n",
        "try:\n",
        "    import torch  \n",
        "    print('Module torch was installed')\n",
        "except ImportError:    \n",
        "    print(\"Installinng torch 1.8.1\")\n",
        "    !pip install -q torch==1.8.1\n",
        "try:\n",
        "    import torchvision  \n",
        "    print('Module torchvision was installed')\n",
        "except ImportError:    \n",
        "    print(\"Installinng torchvision 0.4.0\")\n",
        "    !pip install -q torchvision==0.4.0\n",
        "    \n",
        "try:\n",
        "    import qiskit  \n",
        "    print('Module qiskit was installed')\n",
        "except ImportError:    \n",
        "    print(\"Installinng qiskit 0.29.0\")\n",
        "    !pip install -q qiskit==0.29.0\n",
        "import sys\n",
        "if is_colab:\n",
        "    !pip install pylatexenc\n",
        "    !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1VYR5kM8oCGaMXPIq0K1UY7u53a77vmHA' -O codes.tar.gz\n",
        "    !tar zxvf /content/codes.tar.gz\n",
        "    sys.path.append('/content/libs/')\n",
        "\n",
        "#define parameter\n",
        "input_data_num = 100\n",
        "init_weight = torch.tensor([[-1,-1,1,1,1,1,1,1]],dtype=torch.double)\n",
        "threshold = 0.2\n",
        "qubit_num = 3\n",
        "is_generate_data = False"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Module torch was installed\n",
            "Module torchvision was installed\n",
            "Module qiskit was installed\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0DDSuYoBURg",
        "outputId": "6ca7abd9-23d7-491a-cbbb-01b661ce718a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import sys\n",
        "import functools\n",
        "from qiskit import  QuantumCircuit, ClassicalRegister\n",
        "from qiskit import Aer, execute\n",
        "from qiskit.circuit import Qubit\n",
        "import math\n",
        "\n",
        "from circuit.lib_qiskit_commons import *\n",
        "from circuit.lib_circuit_linner_square import *\n",
        "from training.lib_dataloader import *\n",
        "\n",
        "import qiskit\n",
        "\n",
        "print = functools.partial(print, flush=True)\n",
        "\n",
        "from qiskit import Aer, execute,IBMQ,transpile\n",
        "from qiskit.providers.aer.noise.errors import pauli_error, depolarizing_error\n",
        "from qiskit.providers.aer.noise import NoiseModel\n",
        "from qiskit.visualization import *\n",
        "from qiskit import circuit\n",
        "\n",
        "\n",
        "####TODO:change simulator,add errors.####\n",
        "def my_ibmq(circuit,shots,Simulation = True,backend_name='ibmq_essex'):     \n",
        "    if not Simulation:\n",
        "        provider = IBMQ.get_provider('ibm-q-academic')\n",
        "        backend = provider.get_backend(backend_name)\n",
        "    else:\n",
        "        backend = Aer.get_backend('qasm_simulator')\n",
        "    # circuit.save_statevector()\n",
        "    p_gate = 0.01\n",
        "    x_error_1 = pauli_error([('X',p_gate), ('I', 1 - p_gate)])\n",
        "    z_error_1 = pauli_error([('Z',p_gate), ('I', 1 - p_gate)])\n",
        "\n",
        "    error_1 = x_error_1.compose(z_error_1)\n",
        "    error_2 = error_1.tensor(error_1)\n",
        "    error_3 = error_2.tensor(error_1)\n",
        "\n",
        "    noise_model = NoiseModel()\n",
        "    noise_model.add_all_qubit_quantum_error(error_1, [\"x\"]) # single qubit gate error is applied to x gates\n",
        "    noise_model.add_all_qubit_quantum_error(error_1, [\"z\"]) # single qubit gate error is applied to x gates\n",
        "\n",
        "    noise_model.add_all_qubit_quantum_error(error_2, [\"cx\"]) # two qubit gate error is applied to cx gates\n",
        "    noise_model.add_all_qubit_quantum_error(error_2, [\"cz\"]) # two qubit gate error is applied to cx gates\n",
        "\n",
        "    noise_model.add_all_qubit_quantum_error(error_3, [\"ccx\"])\n",
        "\n",
        "    basis_gates = noise_model.basis_gates\n",
        "    job_ibm_q = execute(circuit, backend, shots=shots, basis_gates=basis_gates, noise_model=noise_model)\n",
        "    if not Simulation:\n",
        "        job_monitor(job_ibm_q)\n",
        "    result_ibm_q = job_ibm_q.result()\n",
        "\n",
        "    counts = result_ibm_q.get_counts()\n",
        "    \n",
        "    return counts"
      ],
      "outputs": [],
      "metadata": {
        "id": "te3pPDxLB-2G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "source": [
        "def equa(circuit,in_qubits,out_qubit,aux = []):\n",
        "  for i in range(3):\n",
        "            circuit.h(in_qubits[i])\n",
        "            circuit.x(in_qubits[i])\n",
        "  circuit.barrier()\n",
        "  #first ccx\n",
        "  circuit.cx(in_qubits[0], aux[0])\n",
        "  circuit.cx(in_qubits[0], aux[0])\n",
        "  circuit.cx(in_qubits[1], aux[0])\n",
        "  circuit.cx(in_qubits[1], aux[0])\n",
        "  circuit.cx(in_qubits[0], in_qubits[1])\n",
        "  circuit.cx(in_qubits[0], in_qubits[1])\n",
        "  #second ccx\n",
        "  circuit.cx(in_qubits[2], aux[1])\n",
        "  circuit.cx(in_qubits[2], aux[1])\n",
        "  circuit.cx(aux[0], aux[1])\n",
        "  circuit.cx(aux[0], aux[1])\n",
        "  circuit.cx(in_qubits[2], aux[0])\n",
        "  circuit.cx(in_qubits[2], aux[0])\n",
        "  circuit.cz(aux[1], out_qubit[0])\n",
        "  #third ccx\n",
        "  circuit.cx(in_qubits[2], aux[1])\n",
        "  circuit.cx(in_qubits[2], aux[1])\n",
        "  circuit.cx(aux[0], aux[1])\n",
        "  circuit.cx(aux[0], aux[1])\n",
        "  circuit.cx(in_qubits[2], aux[0])\n",
        "  circuit.cx(in_qubits[2], aux[0])\n",
        "  #fourth ccx\n",
        "  circuit.cx(in_qubits[0], aux[0])\n",
        "  circuit.cx(in_qubits[0], aux[0])\n",
        "  circuit.cx(in_qubits[1], aux[0])\n",
        "  circuit.cx(in_qubits[1], aux[0])\n",
        "  circuit.cx(in_qubits[0], in_qubits[1])\n",
        "  circuit.cx(in_qubits[0], in_qubits[1])        \n"
      ],
      "outputs": [],
      "metadata": {
        "id": "nc4HJ29bb4z_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "source": [
        "def func(circuit,in_qubits,out_qubit,aux = []):\n",
        "\n",
        "  circuit.h(in_qubits)\n",
        "  circuit.x(in_qubits)\n",
        "  circuit.barrier()\n",
        "  #first ccx\n",
        "  circuit.cx(in_qubits[0], aux[0])\n",
        "  circuit.cx(in_qubits[0], aux[0])\n",
        "  circuit.cx(in_qubits[1], aux[0])\n",
        "  circuit.cx(in_qubits[1], aux[0])\n",
        "  circuit.swap(aux[0], in_qubits[1])\n",
        "  circuit.cx(in_qubits[0], in_qubits[1])\n",
        "  circuit.cx(in_qubits[0], in_qubits[1])\n",
        "  #second ccx\n",
        "  circuit.cx(in_qubits[2], aux[1])\n",
        "  circuit.cx(in_qubits[2], aux[1])\n",
        "  circuit.cx(aux[0], aux[1])\n",
        "  circuit.cx(aux[0], aux[1])\n",
        "  circuit.swap(aux[1], in_qubits[2])\n",
        "  circuit.cx(in_qubits[2], aux[0])\n",
        "  circuit.cx(in_qubits[2], aux[0])\n",
        "  circuit.cz(aux[1], out_qubit[0])\n",
        "  #third ccx\n",
        "  circuit.cx(in_qubits[2], aux[1])\n",
        "  circuit.cx(in_qubits[2], aux[1])\n",
        "  circuit.swap(in_qubits[2], aux[1])\n",
        "  circuit.cx(aux[0], aux[1])\n",
        "  circuit.cx(aux[0], aux[1])\n",
        "  circuit.cx(aux[1], in_qubits[2])\n",
        "  circuit.cx(aux[0], aux[1])\n",
        "  circuit.cx(aux[1], in_qubits[2])\n",
        "  circuit.cx(aux[0], aux[1])\n",
        "  circuit.cx(in_qubits[2], aux[0])\n",
        "  circuit.cx(in_qubits[2], aux[0])\n",
        "  #fourth ccx\n",
        "  circuit.swap(in_qubits[1], aux[0])\n",
        "  circuit.cx(in_qubits[0], aux[0])\n",
        "  circuit.cx(in_qubits[0], aux[0])\n",
        "  circuit.cx(in_qubits[1], aux[0])\n",
        "  circuit.cx(in_qubits[1], aux[0])\n",
        "  circuit.cx(aux[0], in_qubits[1])\n",
        "  circuit.cx(in_qubits[0], in_qubits[1])        \n",
        "  circuit.cx(aux[0], in_qubits[1])\n",
        "  circuit.cx(in_qubits[0], in_qubits[1])\n",
        "  circuit.cx(in_qubits[0], in_qubits[1])\n",
        "  circuit.cx(in_qubits[0], in_qubits[1])        \n"
      ],
      "outputs": [],
      "metadata": {
        "id": "3up5epoRFDUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## define parameters"
      ],
      "metadata": {
        "id": "d96sgIzCAGlm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "source": [
        "def sign(input,th = 0.5):\n",
        "    output = input.new(input.size())\n",
        "    output[input >= th] = 1\n",
        "    output[input < th] = -1\n",
        "    return output\n",
        "\n",
        "def get_traverse_weight(n):\n",
        "    sum_mat = []\n",
        "    flag = \"0\"+str(n)+\"b\"\n",
        "    for i in range(0,int(math.pow(2,n))):\n",
        "        bit_str = format(i,flag)\n",
        "        row = []\n",
        "        for c in bit_str:\n",
        "            row.append(float(c))\n",
        "        sum_mat.append(row)\n",
        "    return sum_mat"
      ],
      "outputs": [],
      "metadata": {
        "id": "3_b5893lAGlm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## generate datasets"
      ],
      "metadata": {
        "id": "Lbx54EzVAGlm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "source": [
        "\n",
        "#get correct result with no error\n",
        "if is_generate_data:\n",
        "    input_data = torch.rand(input_data_num,int(math.pow(2,qubit_num)),dtype=torch.double)\n",
        "    input_data_quantum_matrix = []\n",
        "    for i in range(input_data_num):\n",
        "        quantum_matrix =to_quantum_matrix(input_data[i])\n",
        "        # n1_q_gates,n1_idx =ULayerCircuit.extract_from_weight(init_weight[0])\n",
        "        input_data_quantum_matrix.append(quantum_matrix[:, 0].view(-1))\n",
        "    \n",
        "    input_data_quantum_matrix = torch.stack(input_data_quantum_matrix)\n",
        "    # print(input_data_quantum_matrix.t()[0:8].t())\n",
        "    # print(input_data_quantum_matrix)\n",
        "    correct_result =torch.mm(init_weight,input_data_quantum_matrix.t())\n",
        "    correct_result = correct_result*correct_result/math.pow(2,qubit_num)\n",
        "    # print(correct_result)\n",
        "    correct_result_binary = sign(correct_result.t(),threshold)\n",
        "    print(correct_result_binary.t())\n",
        "    torch.save(input_data, \"./liang_random_data.pt\")\n",
        "    torch.save(correct_result_binary.view(-1), \"./liang_random_target.pt\")\n",
        "    torch.save(init_weight, \"./liang_weight.pt\")\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEKOxlfZAGlm",
        "outputId": "0ffcf339-4134-4b8b-c482-868435db3616"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## define forward,including mapping "
      ],
      "metadata": {
        "id": "j1IiYQUuAGln"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "source": [
        "def forward(circuit,quantum_matrix,weight, is_mapping = False):\n",
        "    #generate u-layer\n",
        "    #define your input and output number\n",
        "    u_layer = ULayerCircuit(qubit_num,1) \n",
        "    #init circuit\n",
        "    #add input qubit to your circuit\n",
        "    inps = u_layer.add_input_qubits(circuit)\n",
        "    \n",
        "    aux = u_layer.add_aux(circuit)\n",
        "    \n",
        "    #add output qubit to your circuit\n",
        "    u_layer_out_qubits = u_layer.add_out_qubits(circuit)\n",
        "    \n",
        "    #add ulayer to your circuit\n",
        "    #add ulayer to your circuit\n",
        "    u_layer.add_weight(circuit,weight,inps,quantum_matrix,aux)\n",
        "\n",
        "    ####TODO: add mapping code here ####\n",
        "\n",
        "    ############################\n",
        "    \n",
        "    if is_mapping:\n",
        "        func(circuit,inps[0],u_layer_out_qubits,aux)\n",
        "    else:\n",
        "        u_layer.sum2(circuit,inps,u_layer_out_qubits,aux)\n",
        "    \n",
        "    #measure\n",
        "    c_reg = ClassicalRegister(1,\"reg\")\n",
        "    circuit.add_register(c_reg)\n",
        "    circuit.measure(u_layer_out_qubits[0],c_reg[0])\n",
        "    \n",
        "    #get the result\n",
        "    qc_shots=8192\n",
        "    opt_class_prob = []\n",
        "    opt_counts = my_ibmq(circuit,qc_shots,True)\n",
        "    (opt_mycount,bits) = analyze(opt_counts)\n",
        "    for b in range(bits):\n",
        "        opt_class_prob.append(float(opt_mycount[b])/qc_shots)\n",
        "    return opt_class_prob[0]\n",
        "\n",
        "\n",
        "    #example of u-layer\n",
        "i = 0\n",
        "#transform input_data into u-Mat\n",
        "input_data = torch.load(\"./liang_random_data.pt\")\n",
        "quantum_matrix =to_quantum_matrix(input_data[i])\n",
        "circuit = QuantumCircuit()\n",
        "prob = forward(circuit,quantum_matrix,init_weight, False)\n",
        "print(\"Result :\",prob)\n",
        "circuit.draw('mpl',fold=50)\n",
        "\n",
        "\n",
        "######################################################\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result : 0.1866455078125\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Figure size 969.964x445.48 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"350.7374pt\" version=\"1.1\" viewBox=\"0 0 746.722874 350.7374\" width=\"746.722874pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-18T01:36:48.625484</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 350.7374 \nL 746.722874 350.7374 \nL 746.722874 0 \nL 0 0 \nz\n\" style=\"fill:#ffffff;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 658.165584 305.926697 \nL 669.982844 305.926697 \nL 664.074214 314.44876 \nz\n\" style=\"fill:#778899;\"/>\n   </g>\n   <g id=\"line2d_1\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 185.020674 43.5608 \nL 734.977774 43.5608 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_2\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 185.020674 89.0118 \nL 734.977774 89.0118 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_3\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 185.020674 134.4628 \nL 734.977774 134.4628 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_4\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 185.020674 179.9138 \nL 734.977774 179.9138 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_5\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 185.020674 225.3648 \nL 734.977774 225.3648 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_6\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 185.020674 270.8158 \nL 734.977774 270.8158 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_7\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 185.020674 314.789642 \nL 734.977774 314.789642 \n\" style=\"fill:none;stroke:#778899;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_8\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 185.020674 317.743957 \nL 734.977774 317.743957 \n\" style=\"fill:none;stroke:#778899;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 337.054269 66.2863 \nL 354.780159 66.2863 \nL 354.780159 20.8353 \nL 337.054269 20.8353 \nz\n\" style=\"fill:#bdbdbd;opacity:0.6;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 337.054269 111.7373 \nL 354.780159 111.7373 \nL 354.780159 66.2863 \nL 337.054269 66.2863 \nz\n\" style=\"fill:#bdbdbd;opacity:0.6;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 337.054269 157.1883 \nL 354.780159 157.1883 \nL 354.780159 111.7373 \nL 337.054269 111.7373 \nz\n\" style=\"fill:#bdbdbd;opacity:0.6;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 337.054269 202.6393 \nL 354.780159 202.6393 \nL 354.780159 157.1883 \nL 337.054269 157.1883 \nz\n\" style=\"fill:#bdbdbd;opacity:0.6;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 337.054269 248.0903 \nL 354.780159 248.0903 \nL 354.780159 202.6393 \nL 337.054269 202.6393 \nz\n\" style=\"fill:#bdbdbd;opacity:0.6;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 337.054269 293.5413 \nL 354.780159 293.5413 \nL 354.780159 248.0903 \nL 337.054269 248.0903 \nz\n\" style=\"fill:#bdbdbd;opacity:0.6;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 473.407269 66.2863 \nL 491.133159 66.2863 \nL 491.133159 20.8353 \nL 473.407269 20.8353 \nz\n\" style=\"fill:#bdbdbd;opacity:0.6;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 473.407269 111.7373 \nL 491.133159 111.7373 \nL 491.133159 66.2863 \nL 473.407269 66.2863 \nz\n\" style=\"fill:#bdbdbd;opacity:0.6;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 473.407269 157.1883 \nL 491.133159 157.1883 \nL 491.133159 111.7373 \nL 473.407269 111.7373 \nz\n\" style=\"fill:#bdbdbd;opacity:0.6;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 473.407269 202.6393 \nL 491.133159 202.6393 \nL 491.133159 157.1883 \nL 473.407269 157.1883 \nz\n\" style=\"fill:#bdbdbd;opacity:0.6;\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 473.407269 248.0903 \nL 491.133159 248.0903 \nL 491.133159 202.6393 \nL 473.407269 202.6393 \nz\n\" style=\"fill:#bdbdbd;opacity:0.6;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 473.407269 293.5413 \nL 491.133159 293.5413 \nL 491.133159 248.0903 \nL 473.407269 248.0903 \nz\n\" style=\"fill:#bdbdbd;opacity:0.6;\"/>\n   </g>\n   <g id=\"line2d_9\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 527.721214 179.9138 \nL 527.721214 43.5608 \n\" style=\"fill:none;stroke:#bb8bff;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_10\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 573.172214 225.3648 \nL 573.172214 134.4628 \n\" style=\"fill:none;stroke:#bb8bff;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_11\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 618.623214 270.8158 \nL 618.623214 225.3648 \n\" style=\"fill:none;stroke:#6fa4ff;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_12\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 664.074214 225.3648 \nL 664.074214 134.4628 \n\" style=\"fill:none;stroke:#bb8bff;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 665.551372 270.8158 \nL 665.551372 305.926697 \n\" style=\"fill:none;stroke:#778899;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 662.597057 270.8158 \nL 662.597057 305.926697 \n\" style=\"fill:none;stroke:#778899;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 709.525214 179.9138 \nL 709.525214 43.5608 \n\" style=\"fill:none;stroke:#bb8bff;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 300.466214 89.0118 \nL 300.466214 43.5608 \n\" style=\"fill:none;stroke:#6fa4ff;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 209.902824 149.234375 \nL 254.676604 149.234375 \nL 254.676604 28.789225 \nL 209.902824 28.789225 \nz\n\" style=\"fill:#bb8bff;stroke:#bb8bff;stroke-linejoin:miter;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 300.466214 47.992272 \nC 301.641455 47.992272 302.768718 47.525345 303.599739 46.694324 \nC 304.430759 45.863304 304.897687 44.73604 304.897687 43.5608 \nC 304.897687 42.38556 304.430759 41.258296 303.599739 40.427276 \nC 302.768718 39.596255 301.641455 39.129327 300.466214 39.129327 \nC 299.290974 39.129327 298.16371 39.596255 297.33269 40.427276 \nC 296.50167 41.258296 296.034742 42.38556 296.034742 43.5608 \nC 296.034742 44.73604 296.50167 45.863304 297.33269 46.694324 \nC 298.16371 47.525345 299.290974 47.992272 300.466214 47.992272 \nz\n\" style=\"fill:#6fa4ff;stroke:#6fa4ff;stroke-linejoin:miter;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 300.466214 93.443272 \nC 301.641455 93.443272 302.768718 92.976345 303.599739 92.145324 \nC 304.430759 91.314304 304.897687 90.18704 304.897687 89.0118 \nC 304.897687 87.83656 304.430759 86.709296 303.599739 85.878276 \nC 302.768718 85.047255 301.641455 84.580327 300.466214 84.580327 \nC 299.290974 84.580327 298.16371 85.047255 297.33269 85.878276 \nC 296.50167 86.709296 296.034742 87.83656 296.034742 89.0118 \nC 296.034742 90.18704 296.50167 91.314304 297.33269 92.145324 \nC 298.16371 92.976345 299.290974 93.443272 300.466214 93.443272 \nz\n\" style=\"fill:#6fa4ff;stroke:#6fa4ff;stroke-linejoin:miter;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 376.596639 58.332375 \nL 406.139789 58.332375 \nL 406.139789 28.789225 \nL 376.596639 28.789225 \nz\n\" style=\"fill:#6fa4ff;stroke:#6fa4ff;stroke-linejoin:miter;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 376.596639 103.783375 \nL 406.139789 103.783375 \nL 406.139789 74.240225 \nL 376.596639 74.240225 \nz\n\" style=\"fill:#6fa4ff;stroke:#6fa4ff;stroke-linejoin:miter;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 376.596639 149.234375 \nL 406.139789 149.234375 \nL 406.139789 119.691225 \nL 376.596639 119.691225 \nz\n\" style=\"fill:#6fa4ff;stroke:#6fa4ff;stroke-linejoin:miter;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 422.047639 58.332375 \nL 451.590789 58.332375 \nL 451.590789 28.789225 \nL 422.047639 28.789225 \nz\n\" style=\"fill:#05bab6;stroke:#05bab6;stroke-linejoin:miter;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 422.047639 103.783375 \nL 451.590789 103.783375 \nL 451.590789 74.240225 \nL 422.047639 74.240225 \nz\n\" style=\"fill:#05bab6;stroke:#05bab6;stroke-linejoin:miter;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 422.047639 149.234375 \nL 451.590789 149.234375 \nL 451.590789 119.691225 \nL 422.047639 119.691225 \nz\n\" style=\"fill:#05bab6;stroke:#05bab6;stroke-linejoin:miter;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 527.721214 47.992272 \nC 528.896455 47.992272 530.023718 47.525345 530.854739 46.694324 \nC 531.685759 45.863304 532.152687 44.73604 532.152687 43.5608 \nC 532.152687 42.38556 531.685759 41.258296 530.854739 40.427276 \nC 530.023718 39.596255 528.896455 39.129327 527.721214 39.129327 \nC 526.545974 39.129327 525.41871 39.596255 524.58769 40.427276 \nC 523.75667 41.258296 523.289742 42.38556 523.289742 43.5608 \nC 523.289742 44.73604 523.75667 45.863304 524.58769 46.694324 \nC 525.41871 47.525345 526.545974 47.992272 527.721214 47.992272 \nz\n\" style=\"fill:#bb8bff;stroke:#bb8bff;stroke-linejoin:miter;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 527.721214 93.443272 \nC 528.896455 93.443272 530.023718 92.976345 530.854739 92.145324 \nC 531.685759 91.314304 532.152687 90.18704 532.152687 89.0118 \nC 532.152687 87.83656 531.685759 86.709296 530.854739 85.878276 \nC 530.023718 85.047255 528.896455 84.580327 527.721214 84.580327 \nC 526.545974 84.580327 525.41871 85.047255 524.58769 85.878276 \nC 523.75667 86.709296 523.289742 87.83656 523.289742 89.0118 \nC 523.289742 90.18704 523.75667 91.314304 524.58769 92.145324 \nC 525.41871 92.976345 526.545974 93.443272 527.721214 93.443272 \nz\n\" style=\"fill:#bb8bff;stroke:#bb8bff;stroke-linejoin:miter;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 527.721214 190.253902 \nC 530.463442 190.253902 533.093723 189.164404 535.032771 187.225357 \nC 536.971818 185.286309 538.061317 182.656027 538.061317 179.9138 \nC 538.061317 177.171573 536.971818 174.541291 535.032771 172.602243 \nC 533.093723 170.663196 530.463442 169.573697 527.721214 169.573697 \nC 524.978987 169.573697 522.348705 170.663196 520.409658 172.602243 \nC 518.47061 174.541291 517.381112 177.171573 517.381112 179.9138 \nC 517.381112 182.656027 518.47061 185.286309 520.409658 187.225357 \nC 522.348705 189.164404 524.978987 190.253902 527.721214 190.253902 \nz\n\" style=\"fill:#bb8bff;stroke:#bb8bff;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_27\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 573.172214 138.894272 \nC 574.347455 138.894272 575.474718 138.427345 576.305739 137.596324 \nC 577.136759 136.765304 577.603687 135.63804 577.603687 134.4628 \nC 577.603687 133.28756 577.136759 132.160296 576.305739 131.329276 \nC 575.474718 130.498255 574.347455 130.031327 573.172214 130.031327 \nC 571.996974 130.031327 570.86971 130.498255 570.03869 131.329276 \nC 569.20767 132.160296 568.740742 133.28756 568.740742 134.4628 \nC 568.740742 135.63804 569.20767 136.765304 570.03869 137.596324 \nC 570.86971 138.427345 571.996974 138.894272 573.172214 138.894272 \nz\n\" style=\"fill:#bb8bff;stroke:#bb8bff;stroke-linejoin:miter;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 573.172214 184.345272 \nC 574.347455 184.345272 575.474718 183.878345 576.305739 183.047324 \nC 577.136759 182.216304 577.603687 181.08904 577.603687 179.9138 \nC 577.603687 178.73856 577.136759 177.611296 576.305739 176.780276 \nC 575.474718 175.949255 574.347455 175.482327 573.172214 175.482327 \nC 571.996974 175.482327 570.86971 175.949255 570.03869 176.780276 \nC 569.20767 177.611296 568.740742 178.73856 568.740742 179.9138 \nC 568.740742 181.08904 569.20767 182.216304 570.03869 183.047324 \nC 570.86971 183.878345 571.996974 184.345272 573.172214 184.345272 \nz\n\" style=\"fill:#bb8bff;stroke:#bb8bff;stroke-linejoin:miter;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 573.172214 235.704902 \nC 575.914442 235.704902 578.544723 234.615404 580.483771 232.676357 \nC 582.422818 230.737309 583.512317 228.107027 583.512317 225.3648 \nC 583.512317 222.622573 582.422818 219.992291 580.483771 218.053243 \nC 578.544723 216.114196 575.914442 215.024697 573.172214 215.024697 \nC 570.429987 215.024697 567.799705 216.114196 565.860658 218.053243 \nC 563.92161 219.992291 562.832112 222.622573 562.832112 225.3648 \nC 562.832112 228.107027 563.92161 230.737309 565.860658 232.676357 \nC 567.799705 234.615404 570.429987 235.704902 573.172214 235.704902 \nz\n\" style=\"fill:#bb8bff;stroke:#bb8bff;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 618.623214 229.796272 \nC 619.798455 229.796272 620.925718 229.329345 621.756739 228.498324 \nC 622.587759 227.667304 623.054687 226.54004 623.054687 225.3648 \nC 623.054687 224.18956 622.587759 223.062296 621.756739 222.231276 \nC 620.925718 221.400255 619.798455 220.933327 618.623214 220.933327 \nC 617.447974 220.933327 616.32071 221.400255 615.48969 222.231276 \nC 614.65867 223.062296 614.191742 224.18956 614.191742 225.3648 \nC 614.191742 226.54004 614.65867 227.667304 615.48969 228.498324 \nC 616.32071 229.329345 617.447974 229.796272 618.623214 229.796272 \nz\n\" style=\"fill:#6fa4ff;stroke:#6fa4ff;stroke-linejoin:miter;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 618.623214 281.155902 \nC 621.365442 281.155902 623.995723 280.066404 625.934771 278.127357 \nC 627.873818 276.188309 628.963317 273.558027 628.963317 270.8158 \nC 628.963317 268.073573 627.873818 265.443291 625.934771 263.504243 \nC 623.995723 261.565196 621.365442 260.475697 618.623214 260.475697 \nC 615.880987 260.475697 613.250705 261.565196 611.311658 263.504243 \nC 609.37261 265.443291 608.283112 268.073573 608.283112 270.8158 \nC 608.283112 273.558027 609.37261 276.188309 611.311658 278.127357 \nC 613.250705 280.066404 615.880987 281.155902 618.623214 281.155902 \nz\n\" style=\"fill:#6fa4ff;stroke:#6fa4ff;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_32\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 664.074214 138.894272 \nC 665.249455 138.894272 666.376718 138.427345 667.207739 137.596324 \nC 668.038759 136.765304 668.505687 135.63804 668.505687 134.4628 \nC 668.505687 133.28756 668.038759 132.160296 667.207739 131.329276 \nC 666.376718 130.498255 665.249455 130.031327 664.074214 130.031327 \nC 662.898974 130.031327 661.77171 130.498255 660.94069 131.329276 \nC 660.10967 132.160296 659.642742 133.28756 659.642742 134.4628 \nC 659.642742 135.63804 660.10967 136.765304 660.94069 137.596324 \nC 661.77171 138.427345 662.898974 138.894272 664.074214 138.894272 \nz\n\" style=\"fill:#bb8bff;stroke:#bb8bff;stroke-linejoin:miter;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_33\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 664.074214 184.345272 \nC 665.249455 184.345272 666.376718 183.878345 667.207739 183.047324 \nC 668.038759 182.216304 668.505687 181.08904 668.505687 179.9138 \nC 668.505687 178.73856 668.038759 177.611296 667.207739 176.780276 \nC 666.376718 175.949255 665.249455 175.482327 664.074214 175.482327 \nC 662.898974 175.482327 661.77171 175.949255 660.94069 176.780276 \nC 660.10967 177.611296 659.642742 178.73856 659.642742 179.9138 \nC 659.642742 181.08904 660.10967 182.216304 660.94069 183.047324 \nC 661.77171 183.878345 662.898974 184.345272 664.074214 184.345272 \nz\n\" style=\"fill:#bb8bff;stroke:#bb8bff;stroke-linejoin:miter;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 664.074214 235.704902 \nC 666.816442 235.704902 669.446723 234.615404 671.385771 232.676357 \nC 673.324818 230.737309 674.414317 228.107027 674.414317 225.3648 \nC 674.414317 222.622573 673.324818 219.992291 671.385771 218.053243 \nC 669.446723 216.114196 666.816442 215.024697 664.074214 215.024697 \nC 661.331987 215.024697 658.701705 216.114196 656.762658 218.053243 \nC 654.82361 219.992291 653.734112 222.622573 653.734112 225.3648 \nC 653.734112 228.107027 654.82361 230.737309 656.762658 232.676357 \nC 658.701705 234.615404 661.331987 235.704902 664.074214 235.704902 \nz\n\" style=\"fill:#bb8bff;stroke:#bb8bff;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 649.302639 285.587375 \nL 678.845789 285.587375 \nL 678.845789 256.044225 \nL 649.302639 256.044225 \nz\n\" style=\"stroke:#000000;stroke-linejoin:miter;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 674.414317 275.247272 \nC 674.414317 272.505959 673.324173 269.874118 671.385771 267.935716 \nC 669.447369 265.997314 666.815528 264.90717 664.074214 264.90717 \nC 661.3329 264.90717 658.701059 265.997314 656.762658 267.935716 \nC 654.824256 269.874118 653.734112 272.505959 653.734112 275.247272 \n\" style=\"fill:none;stroke:#ffffff;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_37\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 709.525214 47.992272 \nC 710.700455 47.992272 711.827718 47.525345 712.658739 46.694324 \nC 713.489759 45.863304 713.956687 44.73604 713.956687 43.5608 \nC 713.956687 42.38556 713.489759 41.258296 712.658739 40.427276 \nC 711.827718 39.596255 710.700455 39.129327 709.525214 39.129327 \nC 708.349974 39.129327 707.22271 39.596255 706.39169 40.427276 \nC 705.56067 41.258296 705.093742 42.38556 705.093742 43.5608 \nC 705.093742 44.73604 705.56067 45.863304 706.39169 46.694324 \nC 707.22271 47.525345 708.349974 47.992272 709.525214 47.992272 \nz\n\" style=\"fill:#bb8bff;stroke:#bb8bff;stroke-linejoin:miter;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_38\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 709.525214 93.443272 \nC 710.700455 93.443272 711.827718 92.976345 712.658739 92.145324 \nC 713.489759 91.314304 713.956687 90.18704 713.956687 89.0118 \nC 713.956687 87.83656 713.489759 86.709296 712.658739 85.878276 \nC 711.827718 85.047255 710.700455 84.580327 709.525214 84.580327 \nC 708.349974 84.580327 707.22271 85.047255 706.39169 85.878276 \nC 705.56067 86.709296 705.093742 87.83656 705.093742 89.0118 \nC 705.093742 90.18704 705.56067 91.314304 706.39169 92.145324 \nC 707.22271 92.976345 708.349974 93.443272 709.525214 93.443272 \nz\n\" style=\"fill:#bb8bff;stroke:#bb8bff;stroke-linejoin:miter;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_39\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 709.525214 190.253902 \nC 712.267442 190.253902 714.897723 189.164404 716.836771 187.225357 \nC 718.775818 185.286309 719.865317 182.656027 719.865317 179.9138 \nC 719.865317 177.171573 718.775818 174.541291 716.836771 172.602243 \nC 714.897723 170.663196 712.267442 169.573697 709.525214 169.573697 \nC 706.782987 169.573697 704.152705 170.663196 702.213658 172.602243 \nC 700.27461 174.541291 699.185112 177.171573 699.185112 179.9138 \nC 699.185112 182.656027 700.27461 185.286309 702.213658 187.225357 \nC 704.152705 189.164404 706.782987 190.253902 709.525214 190.253902 \nz\n\" style=\"fill:#bb8bff;stroke:#bb8bff;stroke-linejoin:miter;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 664.074214 275.247272 \nL 674.414317 264.90717 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 345.917214 20.8353 \nL 345.917214 66.2863 \n\" style=\"fill:none;stroke:#000000;stroke-dasharray:3.7,1.6;stroke-dashoffset:0;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 345.917214 66.2863 \nL 345.917214 111.7373 \n\" style=\"fill:none;stroke:#000000;stroke-dasharray:3.7,1.6;stroke-dashoffset:0;\"/>\n   </g>\n   <g id=\"line2d_20\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 345.917214 111.7373 \nL 345.917214 157.1883 \n\" style=\"fill:none;stroke:#000000;stroke-dasharray:3.7,1.6;stroke-dashoffset:0;\"/>\n   </g>\n   <g id=\"line2d_21\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 345.917214 157.1883 \nL 345.917214 202.6393 \n\" style=\"fill:none;stroke:#000000;stroke-dasharray:3.7,1.6;stroke-dashoffset:0;\"/>\n   </g>\n   <g id=\"line2d_22\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 345.917214 202.6393 \nL 345.917214 248.0903 \n\" style=\"fill:none;stroke:#000000;stroke-dasharray:3.7,1.6;stroke-dashoffset:0;\"/>\n   </g>\n   <g id=\"line2d_23\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 345.917214 248.0903 \nL 345.917214 293.5413 \n\" style=\"fill:none;stroke:#000000;stroke-dasharray:3.7,1.6;stroke-dashoffset:0;\"/>\n   </g>\n   <g id=\"line2d_24\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 482.270214 20.8353 \nL 482.270214 66.2863 \n\" style=\"fill:none;stroke:#000000;stroke-dasharray:3.7,1.6;stroke-dashoffset:0;\"/>\n   </g>\n   <g id=\"line2d_25\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 482.270214 66.2863 \nL 482.270214 111.7373 \n\" style=\"fill:none;stroke:#000000;stroke-dasharray:3.7,1.6;stroke-dashoffset:0;\"/>\n   </g>\n   <g id=\"line2d_26\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 482.270214 111.7373 \nL 482.270214 157.1883 \n\" style=\"fill:none;stroke:#000000;stroke-dasharray:3.7,1.6;stroke-dashoffset:0;\"/>\n   </g>\n   <g id=\"line2d_27\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 482.270214 157.1883 \nL 482.270214 202.6393 \n\" style=\"fill:none;stroke:#000000;stroke-dasharray:3.7,1.6;stroke-dashoffset:0;\"/>\n   </g>\n   <g id=\"line2d_28\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 482.270214 202.6393 \nL 482.270214 248.0903 \n\" style=\"fill:none;stroke:#000000;stroke-dasharray:3.7,1.6;stroke-dashoffset:0;\"/>\n   </g>\n   <g id=\"line2d_29\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 482.270214 248.0903 \nL 482.270214 293.5413 \n\" style=\"fill:none;stroke:#000000;stroke-dasharray:3.7,1.6;stroke-dashoffset:0;\"/>\n   </g>\n   <g id=\"line2d_30\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 527.721214 185.82243 \nL 527.721214 174.00517 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_31\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 521.812584 179.9138 \nL 533.629844 179.9138 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_32\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 573.172214 231.27343 \nL 573.172214 219.45617 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_33\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 567.263584 225.3648 \nL 579.080844 225.3648 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_34\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 618.623214 276.72443 \nL 618.623214 264.90717 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_35\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 612.714584 270.8158 \nL 624.531844 270.8158 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 664.074214 231.27343 \nL 664.074214 219.45617 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_37\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 658.165584 225.3648 \nL 669.982844 225.3648 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_38\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 709.525214 185.82243 \nL 709.525214 174.00517 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_39\">\n    <path clip-path=\"url(#p8728358ad5)\" d=\"M 703.616584 179.9138 \nL 715.433844 179.9138 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"text_1\">\n    <g clip-path=\"url(#p8728358ad5)\">\n     <!-- 0 -->\n     <g transform=\"translate(213.084394 47.147987)scale(0.13 -0.13)\">\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-48\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_2\">\n    <g clip-path=\"url(#p8728358ad5)\">\n     <!-- 1 -->\n     <g transform=\"translate(213.084394 92.598987)scale(0.13 -0.13)\">\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-49\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_3\">\n    <g clip-path=\"url(#p8728358ad5)\">\n     <!-- 2 -->\n     <g transform=\"translate(213.084394 138.049987)scale(0.13 -0.13)\">\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-50\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_4\">\n    <g clip-path=\"url(#p8728358ad5)\">\n     <!-- Input -->\n     <g transform=\"translate(220.458387 92.598987)scale(0.13 -0.13)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-73\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"29.492188\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"92.871094\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"156.347656\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"219.726562\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_5\">\n    <g clip-path=\"url(#p8728358ad5)\">\n     <!-- H -->\n     <g transform=\"translate(386.480011 47.147987)scale(0.13 -0.13)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 43.015625 \nL 55.515625 43.015625 \nL 55.515625 72.90625 \nL 65.375 72.90625 \nL 65.375 0 \nL 55.515625 0 \nL 55.515625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-72\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-72\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_6\">\n    <g clip-path=\"url(#p8728358ad5)\">\n     <!-- H -->\n     <g transform=\"translate(386.480011 92.598987)scale(0.13 -0.13)\">\n      <use xlink:href=\"#DejaVuSans-72\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_7\">\n    <g clip-path=\"url(#p8728358ad5)\">\n     <!-- H -->\n     <g transform=\"translate(386.480011 138.049987)scale(0.13 -0.13)\">\n      <use xlink:href=\"#DejaVuSans-72\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_8\">\n    <g clip-path=\"url(#p8728358ad5)\">\n     <!-- X -->\n     <g transform=\"translate(432.366714 47.147987)scale(0.13 -0.13)\">\n      <defs>\n       <path d=\"M 6.296875 72.90625 \nL 16.890625 72.90625 \nL 35.015625 45.796875 \nL 53.21875 72.90625 \nL 63.8125 72.90625 \nL 40.375 37.890625 \nL 65.375 0 \nL 54.78125 0 \nL 34.28125 31 \nL 13.625 0 \nL 2.984375 0 \nL 29 38.921875 \nz\n\" id=\"DejaVuSans-88\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-88\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_9\">\n    <g clip-path=\"url(#p8728358ad5)\">\n     <!-- X -->\n     <g transform=\"translate(432.366714 92.598987)scale(0.13 -0.13)\">\n      <use xlink:href=\"#DejaVuSans-88\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_10\">\n    <g clip-path=\"url(#p8728358ad5)\">\n     <!-- X -->\n     <g transform=\"translate(432.366714 138.049987)scale(0.13 -0.13)\">\n      <use xlink:href=\"#DejaVuSans-88\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_11\">\n    <g clip-path=\"url(#p8728358ad5)\">\n     <!-- 0 -->\n     <g transform=\"translate(675.436964 309.558825)scale(0.104 -0.104)\">\n      <use xlink:href=\"#DejaVuSans-48\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_12\">\n    <g clip-path=\"url(#p8728358ad5)\">\n     <!-- ${u\\_layer0\\_qbit}\\_{0}$ -->\n     <g transform=\"translate(48.205474 47.7858)scale(0.1625 -0.1625)\">\n      <defs>\n       <path d=\"M 6.6875 21.6875 \nL 13.09375 54.6875 \nL 22.125 54.6875 \nL 15.71875 22.015625 \nQ 15.234375 19.625 15.015625 17.921875 \nQ 14.796875 16.21875 14.796875 15.09375 \nQ 14.796875 10.9375 17.328125 8.65625 \nQ 19.875 6.390625 24.515625 6.390625 \nQ 31.734375 6.390625 37 11.265625 \nQ 42.28125 16.15625 43.890625 24.421875 \nL 49.90625 54.6875 \nL 58.890625 54.6875 \nL 48.296875 0 \nL 39.3125 0 \nL 41.109375 8.59375 \nQ 37.3125 3.8125 32.0625 1.1875 \nQ 26.8125 -1.421875 20.90625 -1.421875 \nQ 13.71875 -1.421875 9.71875 2.515625 \nQ 5.71875 6.453125 5.71875 13.484375 \nQ 5.71875 14.9375 5.953125 17.140625 \nQ 6.203125 19.34375 6.6875 21.6875 \nz\n\" id=\"DejaVuSans-Oblique-117\"/>\n       <path d=\"M 50.984375 -16.609375 \nL 50.984375 -23.578125 \nL -0.984375 -23.578125 \nL -0.984375 -16.609375 \nz\n\" id=\"DejaVuSans-95\"/>\n       <path d=\"M 18.3125 75.984375 \nL 27.296875 75.984375 \nL 12.5 0 \nL 3.515625 0 \nz\n\" id=\"DejaVuSans-Oblique-108\"/>\n       <path d=\"M 53.71875 31.203125 \nL 47.609375 0 \nL 38.625 0 \nL 40.28125 8.296875 \nQ 36.328125 3.421875 31.265625 1 \nQ 26.21875 -1.421875 20.015625 -1.421875 \nQ 13.03125 -1.421875 8.5625 2.84375 \nQ 4.109375 7.125 4.109375 13.8125 \nQ 4.109375 23.390625 11.75 28.953125 \nQ 19.390625 34.515625 32.8125 34.515625 \nL 45.3125 34.515625 \nL 45.796875 36.921875 \nQ 45.90625 37.3125 45.953125 37.765625 \nQ 46 38.234375 46 39.203125 \nQ 46 43.5625 42.453125 45.96875 \nQ 38.921875 48.390625 32.515625 48.390625 \nQ 28.125 48.390625 23.5 47.265625 \nQ 18.890625 46.140625 14.015625 43.890625 \nL 15.578125 52.203125 \nQ 20.65625 54.109375 25.515625 55.046875 \nQ 30.375 56 34.90625 56 \nQ 44.578125 56 49.625 51.796875 \nQ 54.6875 47.609375 54.6875 39.59375 \nQ 54.6875 37.984375 54.4375 35.8125 \nQ 54.203125 33.640625 53.71875 31.203125 \nz\nM 44 27.484375 \nL 35.015625 27.484375 \nQ 23.96875 27.484375 18.671875 24.53125 \nQ 13.375 21.578125 13.375 15.375 \nQ 13.375 11.078125 16.078125 8.640625 \nQ 18.796875 6.203125 23.578125 6.203125 \nQ 30.90625 6.203125 36.375 11.453125 \nQ 41.84375 16.703125 43.609375 25.484375 \nz\n\" id=\"DejaVuSans-Oblique-97\"/>\n       <path d=\"M 24.8125 -5.078125 \nQ 18.5625 -15.578125 14.625 -18.1875 \nQ 10.6875 -20.796875 4.59375 -20.796875 \nL -2.484375 -20.796875 \nL -0.984375 -13.28125 \nL 4.203125 -13.28125 \nQ 7.953125 -13.28125 10.59375 -11.234375 \nQ 13.234375 -9.1875 16.5 -3.21875 \nL 19.28125 2 \nL 7.171875 54.6875 \nL 16.703125 54.6875 \nL 25.78125 12.796875 \nL 50.875 54.6875 \nL 60.296875 54.6875 \nz\n\" id=\"DejaVuSans-Oblique-121\"/>\n       <path d=\"M 48.09375 32.234375 \nQ 48.25 33.015625 48.3125 33.84375 \nQ 48.390625 34.671875 48.390625 35.5 \nQ 48.390625 41.453125 44.890625 44.921875 \nQ 41.40625 48.390625 35.40625 48.390625 \nQ 28.71875 48.390625 23.578125 44.15625 \nQ 18.453125 39.9375 15.828125 32.171875 \nz\nM 55.90625 25.203125 \nL 14.109375 25.203125 \nQ 13.8125 23.34375 13.71875 22.265625 \nQ 13.625 21.1875 13.625 20.40625 \nQ 13.625 13.625 17.796875 9.90625 \nQ 21.96875 6.203125 29.59375 6.203125 \nQ 35.453125 6.203125 40.671875 7.515625 \nQ 45.90625 8.84375 50.390625 11.375 \nL 48.6875 2.484375 \nQ 43.84375 0.53125 38.6875 -0.4375 \nQ 33.546875 -1.421875 28.21875 -1.421875 \nQ 16.84375 -1.421875 10.71875 4.015625 \nQ 4.59375 9.46875 4.59375 19.484375 \nQ 4.59375 28.03125 7.640625 35.375 \nQ 10.6875 42.71875 16.609375 48.484375 \nQ 20.40625 52.09375 25.65625 54.046875 \nQ 30.90625 56 36.8125 56 \nQ 46.09375 56 51.578125 50.4375 \nQ 57.078125 44.875 57.078125 35.5 \nQ 57.078125 33.25 56.78125 30.6875 \nQ 56.5 28.125 55.90625 25.203125 \nz\n\" id=\"DejaVuSans-Oblique-101\"/>\n       <path d=\"M 44.578125 46.390625 \nQ 43.21875 47.125 41.453125 47.515625 \nQ 39.703125 47.90625 37.703125 47.90625 \nQ 30.515625 47.90625 25.140625 42.453125 \nQ 19.78125 37.015625 18.015625 27.875 \nL 12.5 0 \nL 3.515625 0 \nL 14.203125 54.6875 \nL 23.1875 54.6875 \nL 21.484375 46.1875 \nQ 25.046875 50.921875 30 53.453125 \nQ 34.96875 56 40.578125 56 \nQ 42.046875 56 43.453125 55.828125 \nQ 44.875 55.671875 46.296875 55.28125 \nz\n\" id=\"DejaVuSans-Oblique-114\"/>\n       <path d=\"M 41.703125 8.203125 \nQ 38.09375 3.46875 33.171875 1.015625 \nQ 28.265625 -1.421875 22.3125 -1.421875 \nQ 14.015625 -1.421875 9.296875 4.171875 \nQ 4.59375 9.765625 4.59375 19.578125 \nQ 4.59375 27.484375 7.5 34.859375 \nQ 10.40625 42.234375 15.828125 48.09375 \nQ 19.34375 51.90625 23.90625 53.953125 \nQ 28.46875 56 33.5 56 \nQ 39.546875 56 43.453125 53.609375 \nQ 47.359375 51.21875 49.125 46.390625 \nL 50.6875 54.59375 \nL 59.71875 54.59375 \nL 45.125 -20.609375 \nL 36.078125 -20.609375 \nz\nM 13.921875 20.90625 \nQ 13.921875 13.671875 16.9375 9.890625 \nQ 19.96875 6.109375 25.6875 6.109375 \nQ 34.1875 6.109375 40.1875 14.234375 \nQ 46.1875 22.359375 46.1875 33.984375 \nQ 46.1875 41.015625 43.078125 44.75 \nQ 39.984375 48.484375 34.1875 48.484375 \nQ 29.9375 48.484375 26.3125 46.5 \nQ 22.703125 44.53125 20.015625 40.71875 \nQ 17.1875 36.71875 15.546875 31.34375 \nQ 13.921875 25.984375 13.921875 20.90625 \nz\n\" id=\"DejaVuSans-Oblique-113\"/>\n       <path d=\"M 49.515625 33.40625 \nQ 49.515625 40.484375 46.265625 44.484375 \nQ 43.015625 48.484375 37.3125 48.484375 \nQ 33.15625 48.484375 29.515625 46.453125 \nQ 25.875 44.4375 23.1875 40.578125 \nQ 20.359375 36.53125 18.71875 31.171875 \nQ 17.09375 25.828125 17.09375 20.515625 \nQ 17.09375 13.765625 20.28125 9.9375 \nQ 23.484375 6.109375 29.109375 6.109375 \nQ 33.34375 6.109375 36.953125 8.078125 \nQ 40.578125 10.0625 43.3125 13.921875 \nQ 46.09375 17.921875 47.796875 23.234375 \nQ 49.515625 28.5625 49.515625 33.40625 \nz\nM 21.578125 46.390625 \nQ 24.90625 50.875 29.90625 53.4375 \nQ 34.90625 56 40.375 56 \nQ 48.78125 56 53.734375 50.328125 \nQ 58.6875 44.671875 58.6875 35.015625 \nQ 58.6875 27.09375 55.78125 19.671875 \nQ 52.875 12.25 47.515625 6.5 \nQ 44 2.6875 39.40625 0.625 \nQ 34.8125 -1.421875 29.78125 -1.421875 \nQ 24.46875 -1.421875 20.5625 1.015625 \nQ 16.65625 3.46875 14.203125 8.296875 \nL 12.59375 0 \nL 3.609375 0 \nL 18.40625 75.984375 \nL 27.390625 75.984375 \nz\n\" id=\"DejaVuSans-Oblique-98\"/>\n       <path d=\"M 18.3125 75.984375 \nL 27.296875 75.984375 \nL 25.09375 64.59375 \nL 16.109375 64.59375 \nz\nM 14.203125 54.6875 \nL 23.1875 54.6875 \nL 12.5 0 \nL 3.515625 0 \nz\n\" id=\"DejaVuSans-Oblique-105\"/>\n       <path d=\"M 42.28125 54.6875 \nL 40.921875 47.703125 \nL 23 47.703125 \nL 17.1875 18.015625 \nQ 16.890625 16.359375 16.75 15.234375 \nQ 16.609375 14.109375 16.609375 13.484375 \nQ 16.609375 10.359375 18.484375 8.9375 \nQ 20.359375 7.515625 24.515625 7.515625 \nL 33.59375 7.515625 \nL 32.078125 0 \nL 23.484375 0 \nQ 15.484375 0 11.546875 3.125 \nQ 7.625 6.25 7.625 12.59375 \nQ 7.625 13.71875 7.765625 15.0625 \nQ 7.90625 16.40625 8.203125 18.015625 \nL 14.015625 47.703125 \nL 6.390625 47.703125 \nL 7.8125 54.6875 \nL 15.28125 54.6875 \nL 18.3125 70.21875 \nL 27.296875 70.21875 \nL 24.3125 54.6875 \nz\n\" id=\"DejaVuSans-Oblique-116\"/>\n      </defs>\n      <use transform=\"translate(0 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-117\"/>\n      <use transform=\"translate(63.378906 0.015625)\" xlink:href=\"#DejaVuSans-95\"/>\n      <use transform=\"translate(113.378906 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-108\"/>\n      <use transform=\"translate(141.162109 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-97\"/>\n      <use transform=\"translate(202.441406 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-121\"/>\n      <use transform=\"translate(261.621094 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-101\"/>\n      <use transform=\"translate(323.144531 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-114\"/>\n      <use transform=\"translate(364.257812 0.015625)\" xlink:href=\"#DejaVuSans-48\"/>\n      <use transform=\"translate(427.880859 0.015625)\" xlink:href=\"#DejaVuSans-95\"/>\n      <use transform=\"translate(477.880859 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-113\"/>\n      <use transform=\"translate(541.357422 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-98\"/>\n      <use transform=\"translate(604.833984 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-105\"/>\n      <use transform=\"translate(632.617188 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-116\"/>\n      <use transform=\"translate(671.826172 0.015625)\" xlink:href=\"#DejaVuSans-95\"/>\n      <use transform=\"translate(721.826172 0.015625)\" xlink:href=\"#DejaVuSans-48\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_13\">\n    <g clip-path=\"url(#p8728358ad5)\">\n     <!-- ${u\\_layer0\\_qbit}\\_{1}$ -->\n     <g transform=\"translate(48.205474 93.2368)scale(0.1625 -0.1625)\">\n      <use transform=\"translate(0 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-117\"/>\n      <use transform=\"translate(63.378906 0.015625)\" xlink:href=\"#DejaVuSans-95\"/>\n      <use transform=\"translate(113.378906 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-108\"/>\n      <use transform=\"translate(141.162109 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-97\"/>\n      <use transform=\"translate(202.441406 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-121\"/>\n      <use transform=\"translate(261.621094 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-101\"/>\n      <use transform=\"translate(323.144531 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-114\"/>\n      <use transform=\"translate(364.257812 0.015625)\" xlink:href=\"#DejaVuSans-48\"/>\n      <use transform=\"translate(427.880859 0.015625)\" xlink:href=\"#DejaVuSans-95\"/>\n      <use transform=\"translate(477.880859 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-113\"/>\n      <use transform=\"translate(541.357422 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-98\"/>\n      <use transform=\"translate(604.833984 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-105\"/>\n      <use transform=\"translate(632.617188 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-116\"/>\n      <use transform=\"translate(671.826172 0.015625)\" xlink:href=\"#DejaVuSans-95\"/>\n      <use transform=\"translate(721.826172 0.015625)\" xlink:href=\"#DejaVuSans-49\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_14\">\n    <g clip-path=\"url(#p8728358ad5)\">\n     <!-- ${u\\_layer0\\_qbit}\\_{2}$ -->\n     <g transform=\"translate(48.205474 138.6878)scale(0.1625 -0.1625)\">\n      <use transform=\"translate(0 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-117\"/>\n      <use transform=\"translate(63.378906 0.015625)\" xlink:href=\"#DejaVuSans-95\"/>\n      <use transform=\"translate(113.378906 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-108\"/>\n      <use transform=\"translate(141.162109 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-97\"/>\n      <use transform=\"translate(202.441406 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-121\"/>\n      <use transform=\"translate(261.621094 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-101\"/>\n      <use transform=\"translate(323.144531 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-114\"/>\n      <use transform=\"translate(364.257812 0.015625)\" xlink:href=\"#DejaVuSans-48\"/>\n      <use transform=\"translate(427.880859 0.015625)\" xlink:href=\"#DejaVuSans-95\"/>\n      <use transform=\"translate(477.880859 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-113\"/>\n      <use transform=\"translate(541.357422 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-98\"/>\n      <use transform=\"translate(604.833984 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-105\"/>\n      <use transform=\"translate(632.617188 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-116\"/>\n      <use transform=\"translate(671.826172 0.015625)\" xlink:href=\"#DejaVuSans-95\"/>\n      <use transform=\"translate(721.826172 0.015625)\" xlink:href=\"#DejaVuSans-50\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_15\">\n    <g clip-path=\"url(#p8728358ad5)\">\n     <!-- ${aux\\_qbit}\\_{0}$ -->\n     <g transform=\"translate(87.855474 184.1388)scale(0.1625 -0.1625)\">\n      <defs>\n       <path d=\"M 60.015625 54.6875 \nL 34.90625 27.875 \nL 50.296875 0 \nL 39.984375 0 \nL 28.421875 21.6875 \nL 8.296875 0 \nL -2.59375 0 \nL 24.3125 28.8125 \nL 10.015625 54.6875 \nL 20.3125 54.6875 \nL 30.8125 34.90625 \nL 49.125 54.6875 \nz\n\" id=\"DejaVuSans-Oblique-120\"/>\n      </defs>\n      <use transform=\"translate(0 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-97\"/>\n      <use transform=\"translate(61.279297 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-117\"/>\n      <use transform=\"translate(124.658203 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-120\"/>\n      <use transform=\"translate(183.837891 0.015625)\" xlink:href=\"#DejaVuSans-95\"/>\n      <use transform=\"translate(233.837891 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-113\"/>\n      <use transform=\"translate(297.314453 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-98\"/>\n      <use transform=\"translate(360.791016 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-105\"/>\n      <use transform=\"translate(388.574219 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-116\"/>\n      <use transform=\"translate(427.783203 0.015625)\" xlink:href=\"#DejaVuSans-95\"/>\n      <use transform=\"translate(477.783203 0.015625)\" xlink:href=\"#DejaVuSans-48\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_16\">\n    <g clip-path=\"url(#p8728358ad5)\">\n     <!-- ${aux\\_qbit}\\_{1}$ -->\n     <g transform=\"translate(87.855474 229.5898)scale(0.1625 -0.1625)\">\n      <use transform=\"translate(0 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-97\"/>\n      <use transform=\"translate(61.279297 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-117\"/>\n      <use transform=\"translate(124.658203 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-120\"/>\n      <use transform=\"translate(183.837891 0.015625)\" xlink:href=\"#DejaVuSans-95\"/>\n      <use transform=\"translate(233.837891 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-113\"/>\n      <use transform=\"translate(297.314453 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-98\"/>\n      <use transform=\"translate(360.791016 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-105\"/>\n      <use transform=\"translate(388.574219 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-116\"/>\n      <use transform=\"translate(427.783203 0.015625)\" xlink:href=\"#DejaVuSans-95\"/>\n      <use transform=\"translate(477.783203 0.015625)\" xlink:href=\"#DejaVuSans-49\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_17\">\n    <g clip-path=\"url(#p8728358ad5)\">\n     <!-- ${u\\_layer\\_output\\_qbit}\\_{0}$ -->\n     <g transform=\"translate(-3.144526 275.0408)scale(0.1625 -0.1625)\">\n      <defs>\n       <path d=\"M 25.390625 -1.421875 \nQ 15.765625 -1.421875 10.171875 4.515625 \nQ 4.59375 10.453125 4.59375 20.703125 \nQ 4.59375 26.65625 6.515625 32.828125 \nQ 8.453125 39.015625 11.53125 43.21875 \nQ 16.359375 49.75 22.3125 52.875 \nQ 28.265625 56 35.796875 56 \nQ 45.125 56 50.859375 50.1875 \nQ 56.59375 44.390625 56.59375 35.015625 \nQ 56.59375 28.515625 54.6875 22.0625 \nQ 52.78125 15.625 49.703125 11.375 \nQ 44.921875 4.828125 38.96875 1.703125 \nQ 33.015625 -1.421875 25.390625 -1.421875 \nz\nM 13.921875 21 \nQ 13.921875 13.578125 17.015625 9.890625 \nQ 20.125 6.203125 26.421875 6.203125 \nQ 35.453125 6.203125 41.375 14.078125 \nQ 47.3125 21.96875 47.3125 34.078125 \nQ 47.3125 41.15625 44.140625 44.765625 \nQ 40.96875 48.390625 34.8125 48.390625 \nQ 29.734375 48.390625 25.78125 46.015625 \nQ 21.828125 43.65625 18.703125 38.8125 \nQ 16.40625 35.203125 15.15625 30.5625 \nQ 13.921875 25.921875 13.921875 21 \nz\n\" id=\"DejaVuSans-Oblique-111\"/>\n       <path d=\"M 49.609375 33.6875 \nQ 49.609375 40.875 46.484375 44.671875 \nQ 43.359375 48.484375 37.5 48.484375 \nQ 33.5 48.484375 29.859375 46.4375 \nQ 26.21875 44.390625 23.390625 40.484375 \nQ 20.609375 36.625 18.9375 31.15625 \nQ 17.28125 25.6875 17.28125 20.3125 \nQ 17.28125 13.484375 20.40625 9.796875 \nQ 23.53125 6.109375 29.296875 6.109375 \nQ 33.546875 6.109375 37.1875 8.109375 \nQ 40.828125 10.109375 43.40625 13.921875 \nQ 46.1875 17.921875 47.890625 23.34375 \nQ 49.609375 28.765625 49.609375 33.6875 \nz\nM 21.78125 46.390625 \nQ 25.390625 51.125 30.296875 53.5625 \nQ 35.203125 56 41.21875 56 \nQ 49.609375 56 54.25 50.5 \nQ 58.890625 45.015625 58.890625 35.109375 \nQ 58.890625 27 56 19.65625 \nQ 53.125 12.3125 47.703125 6.5 \nQ 44.09375 2.640625 39.546875 0.609375 \nQ 35.015625 -1.421875 29.984375 -1.421875 \nQ 24.171875 -1.421875 20.21875 1 \nQ 16.265625 3.421875 14.3125 8.203125 \nL 8.6875 -20.796875 \nL -0.296875 -20.796875 \nL 14.40625 54.6875 \nL 23.390625 54.6875 \nz\n\" id=\"DejaVuSans-Oblique-112\"/>\n      </defs>\n      <use transform=\"translate(0 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-117\"/>\n      <use transform=\"translate(63.378906 0.015625)\" xlink:href=\"#DejaVuSans-95\"/>\n      <use transform=\"translate(113.378906 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-108\"/>\n      <use transform=\"translate(141.162109 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-97\"/>\n      <use transform=\"translate(202.441406 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-121\"/>\n      <use transform=\"translate(261.621094 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-101\"/>\n      <use transform=\"translate(323.144531 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-114\"/>\n      <use transform=\"translate(364.257812 0.015625)\" xlink:href=\"#DejaVuSans-95\"/>\n      <use transform=\"translate(414.257812 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-111\"/>\n      <use transform=\"translate(475.439453 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-117\"/>\n      <use transform=\"translate(538.818359 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-116\"/>\n      <use transform=\"translate(578.027344 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-112\"/>\n      <use transform=\"translate(641.503906 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-117\"/>\n      <use transform=\"translate(704.882812 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-116\"/>\n      <use transform=\"translate(744.091797 0.015625)\" xlink:href=\"#DejaVuSans-95\"/>\n      <use transform=\"translate(794.091797 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-113\"/>\n      <use transform=\"translate(857.568359 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-98\"/>\n      <use transform=\"translate(921.044922 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-105\"/>\n      <use transform=\"translate(948.828125 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-116\"/>\n      <use transform=\"translate(988.037109 0.015625)\" xlink:href=\"#DejaVuSans-95\"/>\n      <use transform=\"translate(1038.037109 0.015625)\" xlink:href=\"#DejaVuSans-48\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_18\">\n    <g clip-path=\"url(#p8728358ad5)\">\n     <!-- reg -->\n     <g transform=\"translate(149.300787 320.750784)scale(0.1625 -0.1625)\">\n      <defs>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"38.863281\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.386719\" xlink:href=\"#DejaVuSans-103\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p8728358ad5\">\n   <rect height=\"336.3374\" width=\"732.322874\" x=\"7.2\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "image/png": "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"
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "QVp-mYN_AGln",
        "outputId": "1f04b6ec-85ac-4301-9a80-572a377f072d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nas "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "source": [
        "import logging\n",
        "import csv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import random\n",
        "from datetime import datetime\n",
        "import time\n",
        "import torch\n",
        "import os\n",
        "\n",
        "def ema(values):\n",
        "    \"\"\"\n",
        "    Helper function for keeping track of an exponential moving average of a list of values.\n",
        "    For this module, we use it to maintain an exponential moving average of rewards\n",
        "    \"\"\"\n",
        "    weights = np.exp(np.linspace(-1., 0., len(values)))\n",
        "    weights /= weights.sum()\n",
        "    a = np.convolve(values, weights, mode=\"full\")[:len(values)]\n",
        "    return a[-1]\n",
        "\n",
        "class Controller(object):\n",
        "    def __init__(self,config,logger):\n",
        "        self.graph = tf.Graph()\n",
        "\n",
        "        self.logger = logger\n",
        "        config = tf.ConfigProto()\n",
        "        config.gpu_options.allow_growth = True\n",
        "        self.sess = tf.Session(config=config, graph=self.graph)\n",
        "\n",
        "        self.hidden_units = controller_params['hidden_units']\n",
        "\n",
        "        self.nn1_search_space = controller_params['sw_space']\n",
        "\n",
        "        self.nn1_num_para = len(self.nn1_search_space)\n",
        "\n",
        "\n",
        "        self.num_para = self.nn1_num_para\n",
        "\n",
        "        self.nn1_beg, self.nn1_end = 0, self.nn1_num_para\n",
        "\n",
        "        self.para_2_val = {}\n",
        "        idx = 0\n",
        "        for hp in self.nn1_search_space:\n",
        "            self.para_2_val[idx] = hp\n",
        "            idx += 1\n",
        "\n",
        "\n",
        "\n",
        "        self.RNN_classifier = {}\n",
        "        self.RNN_pred_prob = {}\n",
        "        with self.graph.as_default():\n",
        "            self.build_controller()\n",
        "\n",
        "        self.reward_history = []\n",
        "        self.architecture_history = []\n",
        "        self.trained_network = {}\n",
        "\n",
        "        self.explored_info = {}\n",
        "\n",
        "    def build_controller(self):\n",
        "        self.logger.info('Building RNN Network')\n",
        "        # Build inputs and placeholders\n",
        "        with tf.name_scope('controller_inputs'):\n",
        "            # Input to the NASCell\n",
        "            self.child_network_paras = tf.placeholder(tf.int64, [None, self.num_para], name='controller_input')\n",
        "            # Discounted rewards\n",
        "            self.discounted_rewards = tf.placeholder(tf.float32, (None,), name='discounted_rewards')\n",
        "            # WW 12-18: input: the batch_size variable will be used to determine the RNN batch\n",
        "            self.batch_size = tf.placeholder(tf.int32, [], name='batch_size')\n",
        "\n",
        "        with tf.name_scope('embedding'):\n",
        "            self.embedding_weights = []\n",
        "            # share embedding weights for each type of parameters\n",
        "            embedding_id = 0\n",
        "            para_2_emb_id = {}\n",
        "            for i in range(len(self.para_2_val.keys())):\n",
        "                additional_para_size = len(self.para_2_val[i])\n",
        "                additional_para_weights = tf.get_variable('state_embeddings_%d' % (embedding_id),\n",
        "                                                          shape=[additional_para_size, self.hidden_units],\n",
        "                                                          initializer=tf.initializers.random_uniform(-1., 1.))\n",
        "                self.embedding_weights.append(additional_para_weights)\n",
        "                para_2_emb_id[i] = embedding_id\n",
        "                embedding_id += 1\n",
        "\n",
        "            self.embedded_input_list = []\n",
        "            for i in range(self.num_para):\n",
        "                self.embedded_input_list.append(\n",
        "                    tf.nn.embedding_lookup(self.embedding_weights[para_2_emb_id[i]], self.child_network_paras[:, i]))\n",
        "            self.embedded_input = tf.stack(self.embedded_input_list, axis=-1)\n",
        "            self.embedded_input = tf.transpose(self.embedded_input, perm=[0, 2, 1])\n",
        "\n",
        "        logger.info('Building Controller')\n",
        "        with tf.name_scope('controller'):\n",
        "            with tf.variable_scope('RNN'):\n",
        "                nas = tf.contrib.rnn.NASCell(self.hidden_units)\n",
        "                tmp_state = nas.zero_state(batch_size=self.batch_size, dtype=tf.float32)\n",
        "                init_state = tf.nn.rnn_cell.LSTMStateTuple(tmp_state[0], tmp_state[1])\n",
        "\n",
        "                output, final_state = tf.nn.dynamic_rnn(nas, self.embedded_input, initial_state=init_state,\n",
        "                                                        dtype=tf.float32)\n",
        "                tmp_list = []\n",
        "                # print(\"output\",\"=\"*50,output)\n",
        "                # print(\"output slice\",\"=\"*50,output[:,-1,:])\n",
        "                for para_idx in range(self.num_para):\n",
        "                    o = output[:, para_idx, :]\n",
        "                    para_len = len(self.para_2_val[para_idx])\n",
        "                    # len(self.para_val[para_idx % self.para_per_layer])\n",
        "                    classifier = tf.layers.dense(o, units=para_len, name='classifier_%d' % (para_idx), reuse=False)\n",
        "                    self.RNN_classifier[para_idx] = classifier\n",
        "                    prob_pred = tf.nn.softmax(classifier)\n",
        "                    self.RNN_pred_prob[para_idx] = prob_pred\n",
        "                    child_para = tf.argmax(prob_pred, axis=-1)\n",
        "                    tmp_list.append(child_para)\n",
        "                self.pred_val = tf.stack(tmp_list, axis=1)\n",
        "\n",
        "        self.logger.info('Building Optimization')\n",
        "        # with tf.name_scope('Optimization'):\n",
        "        # Global Optimization composes all RNNs in one, like NAS, where arch_idx = 0\n",
        "\n",
        "        with tf.name_scope('Optimizer'):\n",
        "            self.global_step = tf.Variable(0, trainable=False)\n",
        "            self.learning_rate = tf.train.exponential_decay(0.99, self.global_step, 50, 0.5, staircase=True)\n",
        "            self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate)\n",
        "        # self.optimizer = tf.train.AdamOptimizer(learning_rate=0.1)\n",
        "        with tf.name_scope('Loss'):\n",
        "            # We seperately compute loss of each predict parameter since the dim of predicting parameters may not be same\n",
        "            for para_idx in range(self.num_para):\n",
        "                if para_idx == 0:\n",
        "                    self.policy_gradient_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                        logits=self.RNN_classifier[para_idx], labels=self.child_network_paras[:, para_idx])\n",
        "                else:\n",
        "                    self.policy_gradient_loss = tf.add(self.policy_gradient_loss,\n",
        "                                                       tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                                                           logits=self.RNN_classifier[para_idx],\n",
        "                                                           labels=self.child_network_paras[:, para_idx]))\n",
        "                # get mean of loss\n",
        "            self.policy_gradient_loss /= self.num_para\n",
        "            self.total_loss = self.policy_gradient_loss\n",
        "            self.gradients = self.optimizer.compute_gradients(self.total_loss)\n",
        "\n",
        "            # Gradients calculated using REINFORCE\n",
        "            for i, (grad, var) in enumerate(self.gradients):\n",
        "                if grad is not None:\n",
        "                    # print(\"aaa\",grad)\n",
        "                    # print(\"aaa\",self.discounted_rewards)\n",
        "                    # sys.exit(0)\n",
        "                    self.gradients[i] = (grad * self.discounted_rewards, var)\n",
        "\n",
        "        with tf.name_scope('Train_RNN'):\n",
        "            # The main training operation. This applies REINFORCE on the weights of the Controller\n",
        "            # self.train_operation[arch_idx][pip_idx] = self.optimizer[arch_idx][pip_idx].apply_gradients(self.gradients[arch_idx][pip_idx], global_step=self.global_step[arch_idx][pip_idx])\n",
        "            # self.train_operation = self.optimizer.minimize(self.total_loss)\n",
        "            self.train_operation = self.optimizer.apply_gradients(self.gradients)\n",
        "            self.update_global_step = tf.assign(self.global_step, self.global_step + 1, name='update_global_step')\n",
        "\n",
        "        self.logger.info('Successfully built controller')\n",
        "\n",
        "    def child_network_translate(self, child_network):\n",
        "        dnn_out = np.zeros_like(child_network)\n",
        "        for para_idx in range(self.num_para):\n",
        "            dnn_out[0][para_idx] = (self.para_2_val[para_idx][child_network[0][para_idx]])\n",
        "        return dnn_out\n",
        "\n",
        "    def generate_child_network(self, child_network_architecture):\n",
        "        with self.graph.as_default():\n",
        "            feed_dict = {\n",
        "                self.child_network_paras: child_network_architecture,\n",
        "                self.batch_size: 1\n",
        "            }\n",
        "            rnn_out = self.sess.run(self.RNN_pred_prob, feed_dict=feed_dict)\n",
        "            predict_child = np.array([[0] * self.num_para])\n",
        "            # random.seed(datetime.now())\n",
        "            for para_idx, prob in rnn_out.items():\n",
        "                predict_child[0][para_idx] = np.random.choice(range(len(self.para_2_val[para_idx])), p=prob[0])\n",
        "            hyperparameters = self.child_network_translate(predict_child)\n",
        "            return predict_child, hyperparameters\n",
        "\n",
        "    def plot_history(self, history, ylim=(-1, 1), title=\"reward\"):\n",
        "        x = list(range(len(history)))\n",
        "        y = history\n",
        "        plt.plot(x, y)\n",
        "        # plt.ylim(ylim)\n",
        "        plt.title(title)\n",
        "        plt.show()\n",
        "    \n",
        "    def set_calc_acc_func(self,func):\n",
        "        self.caculate_err = func\n",
        "\n",
        "    def para2interface_NN(self, Para_NN1):\n",
        "        # Weiwen 01-24: Build NN using explored hyperparamters, return Network\n",
        "        Para_NN1 = torch.tensor(Para_NN1,dtype= torch.double)\n",
        "        return 1 - self.caculate_err(Para_NN1)\n",
        "\n",
        "    def global_train(self):\n",
        "        with self.graph.as_default():\n",
        "            self.sess.run(tf.global_variables_initializer())\n",
        "        step = 0\n",
        "        total_rewards = 0\n",
        "        child_network = np.array([[0] * self.num_para], dtype=np.int64)\n",
        "\n",
        "        for episode in range(controller_params['max_episodes']):\n",
        "            self.logger.info(\n",
        "                '=-=-==-=-==-=-==-=-==-=-==-=-==-=-=>Episode {}<=-=-==-=-==-=-==-=-==-=-==-=-==-=-='.format(episode))\n",
        "            step += 1\n",
        "            episode_reward_buffer = []\n",
        "            arachitecture_batch = []\n",
        "\n",
        "            if episode % 50 == 0 and episode != 0:\n",
        "                print(\"Process:\", str(float(episode) / controller_params['max_episodes'] * 100) + \"%\", file=sys.stderr)\n",
        "                # self.plot_history(self.reward_history, ylim=(min(self.reward_history)-0.01, max(self.reward_history)+0.01))\n",
        "\n",
        "            for sub_child in range(controller_params[\"num_children_per_episode\"]):\n",
        "                # Generate a child network architecture\n",
        "                child_network, hyperparameters = self.generate_child_network(child_network)\n",
        "\n",
        "                DNA_NN1 = child_network[0][self.nn1_beg:self.nn1_end]\n",
        "\n",
        "\n",
        "                Para_NN1 = hyperparameters[0][self.nn1_beg:self.nn1_end]\n",
        "                \n",
        "\n",
        "                str_NN1 = \" \".join(str(x) for x in Para_NN1)\n",
        "                str_NNs = str_NN1\n",
        "\n",
        "\n",
        "\n",
        "                self.logger.info('=====>Step {}/{} in episode {}: HyperParameters: {} <====='.format(sub_child, \\\n",
        "                                                                                                controller_params[\n",
        "                                                                                                    \"num_children_per_episode\"],\n",
        "                                                                                                episode,\n",
        "                                                                                                hyperparameters))\n",
        "\n",
        "                if str_NNs in self.explored_info.keys():\n",
        "                    accuracy = self.explored_info[str_NNs][0]\n",
        "                    reward = self.explored_info[str_NNs][1]\n",
        "                    \n",
        "\n",
        "                else:\n",
        "                    print(Para_NN1)\n",
        "                    accuracy = self.para2interface_NN(Para_NN1)\n",
        "                    reward = accuracy\n",
        "\n",
        "                    self.explored_info[str_NNs] = {}\n",
        "                    self.explored_info[str_NNs][0] = accuracy\n",
        "                    self.explored_info[str_NNs][1] = reward\n",
        "\n",
        "                self.logger.info(\"====================Results=======================\")\n",
        "                self.logger.info(\"--------->NN: {}, Accuracy: {}\".format(str_NNs, accuracy))\n",
        "                self.logger.info(\"--------->Reward: {}\".format(reward))\n",
        "                self.logger.info(\"=\" * 50)\n",
        "\n",
        "                episode_reward_buffer.append(reward)\n",
        "                identified_arch = np.array(\n",
        "                    list(DNA_NN1))\n",
        "                arachitecture_batch.append(identified_arch)\n",
        "\n",
        "            current_reward = np.array(episode_reward_buffer)\n",
        "\n",
        "            mean_reward = np.mean(current_reward)\n",
        "            self.reward_history.append(mean_reward)\n",
        "            self.architecture_history.append(child_network)\n",
        "            total_rewards += mean_reward\n",
        "\n",
        "            baseline = ema(self.reward_history)\n",
        "            last_reward = self.reward_history[-1]\n",
        "            # rewards = current_reward - baseline\n",
        "            rewards = [last_reward - baseline]\n",
        "\n",
        "            feed_dict = {\n",
        "                self.child_network_paras: arachitecture_batch,\n",
        "                self.batch_size: len(arachitecture_batch),\n",
        "                self.discounted_rewards: rewards\n",
        "            }\n",
        "\n",
        "            with self.graph.as_default():\n",
        "                _, _, loss, lr, gs = self.sess.run(\n",
        "                    [self.train_operation, self.update_global_step, self.total_loss, self.learning_rate,\n",
        "                     self.global_step], feed_dict=feed_dict)\n",
        "\n",
        "            self.logger.info('=-=-=-=-=-=>Episode: {} | Loss: {} | LR: {} | Mean R: {} | Reward: {}<=-=-=-=-='.format(\n",
        "                episode, loss, (lr, gs), mean_reward, rewards))\n",
        "\n",
        "\n",
        "        # print('reward_history',self.reward_history)\n",
        "        # print('str_NNs',str_NNs)\n",
        "        # self.plot_history(self.reward_history, ylim=(min(self.reward_history)-0.01, max(self.reward_history)-0.01))\n",
        "\n",
        "        return Para_NN1"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train and test"
      ],
      "metadata": {
        "id": "_3wQjg9vAGlo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "source": [
        "input_data = torch.load(\"./liang_random_data.pt\")\n",
        "target = torch.load(\"./liang_random_target.pt\")\n",
        "target = target.int()\n",
        "correct_weight = torch.load(\"./liang_weight.pt\")\n",
        "print('correct_weight:',correct_weight)\n",
        "\n",
        "train_rate = 0.2\n",
        "input_data_num = target.shape[0]\n",
        "train_num = int(input_data_num *train_rate)\n",
        "is_train = True\n",
        "search_model = 'rl' #random/traverse/rl\n",
        "\n",
        "\n",
        "\n",
        "def calculate_error_rate(weight):\n",
        "    global input_data\n",
        "    global target\n",
        "    global correct_weight\n",
        "    global train_num\n",
        "    \n",
        "    weight = weight.view(1,-1)\n",
        "    circuit_result = []\n",
        "    for i in range(train_num):\n",
        "        quantum_matrix =to_quantum_matrix(input_data[i])\n",
        "        circuit = QuantumCircuit()   \n",
        "        prob = forward(circuit,quantum_matrix,weight,False)\n",
        "        if prob >= threshold :\n",
        "            circuit_result.append(1)\n",
        "        else:\n",
        "            circuit_result.append(-1)\n",
        "    circuit_result_tensor = torch.tensor(circuit_result,dtype=torch.double)\n",
        "    circuit_result_tensor = circuit_result_tensor.int()\n",
        "    error_num = (circuit_result_tensor != target[0:train_num]).sum()\n",
        "    error_rate = float(error_num)/(train_num+0.0000000001)\n",
        "    return error_rate\n",
        "\n",
        "if is_train:\n",
        "    if search_model == 'random':\n",
        "        weight_list = []\n",
        "        acc_list = []\n",
        "        itor_times = 50\n",
        "        for i in range(itor_times):\n",
        "            rand_weight = sign(torch.rand(1,int(math.pow(2,qubit_num)), dtype= torch.double))\n",
        "            acc = 1- calculate_error_rate(rand_weight)\n",
        "            weight_list.append(rand_weight)\n",
        "            acc_list.append(acc)\n",
        "        good_weight = weight_list[acc_list.index(max(acc_list))]\n",
        "        print(\"good_weight:\",good_weight,',highest accuracy:',max(acc_list))\n",
        "\n",
        "    elif search_model == 'traverse':\n",
        "        acc_list = []\n",
        "        traverse_mat = get_traverse_weight(int(math.pow(2,qubit_num)))\n",
        "        traverse_mat_tensor = torch.tensor(traverse_mat,dtype=torch.double)\n",
        "        traverse_weight_tensor = traverse_mat_tensor.new(traverse_mat_tensor.size())\n",
        "        traverse_weight_tensor[traverse_mat_tensor == 0] =-1\n",
        "        traverse_weight_tensor[traverse_mat_tensor == 1] = 1\n",
        "        for i in range(traverse_weight_tensor.shape[0]):\n",
        "            # print(traverse_weight_tensor[i].view(1,-1))\n",
        "            acc = 1- calculate_error_rate(traverse_weight_tensor[i])\n",
        "            acc_list.append(acc)           \n",
        "        good_weight = traverse_weight_tensor[acc_list.index(max(acc_list))]\n",
        "        print(\"good_weight:\",good_weight,',highest accuracy:',max(acc_list))\n",
        "    \n",
        "    elif search_model == 'rl':\n",
        "        logger = logging.getLogger(__name__)\n",
        "        controller_params = {\n",
        "            \"sw_space\": ([-1,+1],[-1,+1],[-1,+1],[-1,+1],[-1,+1],[-1,+1],[-1,+1],[-1,+1]),\n",
        "            # dataflow 1, dataflow 2, PE for d1, BW for d1\n",
        "            'max_episodes': 50,\n",
        "            \"num_children_per_episode\": 1,\n",
        "            'hidden_units': 35,\n",
        "        }\n",
        "\n",
        "        seed = 0\n",
        "        torch.manual_seed(seed)\n",
        "        random.seed(seed)\n",
        "        logging.basicConfig(stream=sys.stdout,\n",
        "                            level=logging.CRITICAL,\n",
        "                            format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s')\n",
        "        \n",
        "        print(\"Nas Begin\")\n",
        "        controller = Controller(controller_params,logger)\n",
        "        controller.set_calc_acc_func(calculate_error_rate)\n",
        "        good_weight = controller.global_train()\n",
        "        good_weight = torch.tensor(good_weight,dtype = torch.double)\n",
        "        print(\"good_weight:\",good_weight)\n",
        "\n",
        "       \n",
        "\n",
        "    #test\n",
        "    circuit_result = []\n",
        "    for i in range(train_num,input_data_num):\n",
        "        quantum_matrix =to_quantum_matrix(input_data[i])\n",
        "    \n",
        "        circuit = QuantumCircuit()\n",
        "        prob = forward(circuit,quantum_matrix,good_weight.view(1,-1),False)\n",
        "    \n",
        "        if prob >= threshold :\n",
        "            circuit_result.append(1)\n",
        "        else:\n",
        "            circuit_result.append(-1)\n",
        "    \n",
        "    circuit_result_tensor = torch.tensor(circuit_result,dtype=torch.double)\n",
        "    circuit_result_tensor = circuit_result_tensor.int()\n",
        "    error_num = (circuit_result_tensor != target[train_num:input_data_num]).sum()\n",
        "    \n",
        "    print(\"error_num :\",error_num)\n",
        "    print(\"error_rate:\",float(error_num)/(input_data_num - train_num))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correct_weight: tensor([[-1., -1.,  1.,  1.,  1.,  1.,  1.,  1.]], dtype=torch.float64)\n",
            "Nas Begin\n",
            "[ 1  1 -1  1  1  1 -1  1]\n",
            "[-1  1 -1 -1 -1  1 -1 -1]\n",
            "[-1 -1  1 -1 -1  1 -1 -1]\n",
            "[-1 -1 -1  1 -1 -1  1  1]\n",
            "[-1 -1  1 -1 -1  1 -1  1]\n",
            "[-1  1  1 -1  1  1  1 -1]\n",
            "[-1  1  1  1  1 -1  1 -1]\n",
            "[-1 -1 -1  1  1  1  1  1]\n",
            "[-1  1  1 -1 -1 -1  1  1]\n",
            "[-1 -1  1  1 -1  1  1 -1]\n",
            "[-1 -1 -1  1 -1  1  1  1]\n",
            "[ 1 -1 -1 -1 -1  1 -1 -1]\n",
            "[-1  1 -1  1  1  1  1 -1]\n",
            "[ 1  1 -1  1 -1 -1 -1 -1]\n",
            "[-1  1 -1 -1 -1  1  1 -1]\n",
            "[-1  1  1  1 -1 -1 -1  1]\n",
            "[ 1  1 -1 -1  1  1  1  1]\n",
            "[-1 -1 -1  1 -1 -1 -1 -1]\n",
            "[-1  1  1 -1  1 -1  1  1]\n",
            "[-1 -1 -1 -1 -1  1 -1  1]\n",
            "[-1  1 -1  1 -1  1  1 -1]\n",
            "[-1 -1  1  1  1  1  1  1]\n",
            "[-1 -1  1  1 -1 -1  1 -1]\n",
            "[-1 -1 -1 -1  1  1  1  1]\n",
            "[-1 -1 -1 -1  1 -1  1  1]\n",
            "[ 1 -1  1 -1 -1 -1  1  1]\n",
            "[ 1  1  1  1  1 -1  1 -1]\n",
            "[-1  1  1  1 -1 -1  1  1]\n",
            "[ 1  1 -1  1  1  1  1  1]\n",
            "[ 1 -1  1 -1 -1  1 -1  1]\n",
            "[ 1 -1  1  1  1 -1 -1  1]\n",
            "[-1 -1  1  1 -1  1 -1  1]\n",
            "[-1 -1  1  1 -1 -1  1  1]\n",
            "[ 1 -1 -1 -1 -1 -1  1 -1]\n",
            "[-1 -1  1 -1 -1  1  1  1]\n",
            "[-1  1  1  1 -1  1  1  1]\n",
            "[-1 -1 -1 -1  1  1 -1  1]\n",
            "[ 1  1 -1 -1 -1  1  1 -1]\n",
            "[ 1  1  1 -1 -1 -1  1 -1]\n",
            "[ 1  1  1  1  1  1  1 -1]\n",
            "[ 1 -1 -1  1 -1  1 -1  1]\n",
            "[-1  1  1  1 -1  1  1 -1]\n",
            "[-1  1 -1 -1 -1 -1  1 -1]\n",
            "[ 1 -1 -1 -1 -1  1  1  1]\n",
            "[-1 -1 -1  1 -1 -1 -1  1]\n",
            "good_weight: tensor([-1., -1.,  1., -1., -1.,  1., -1.,  1.], dtype=torch.float64)\n",
            "error_num : tensor(51)\n",
            "error_rate: 0.6375\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ1IDIj9AGlo",
        "outputId": "45bc07e6-ee39-4725-b078-a68b8b123203"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "source": [
        "#test\n",
        "circuit_result = []\n",
        "for i in range(train_num,input_data_num):\n",
        "    quantum_matrix =to_quantum_matrix(input_data[i])\n",
        "\n",
        "    circuit = QuantumCircuit()   \n",
        "    prob = forward(circuit,quantum_matrix,correct_weight,False)\n",
        "\n",
        "    if prob >= threshold :\n",
        "        circuit_result.append(1)\n",
        "    else:\n",
        "        circuit_result.append(-1)\n",
        "\n",
        "circuit_result_tensor = torch.tensor(circuit_result,dtype=torch.double)\n",
        "circuit_result_tensor = circuit_result_tensor.int()\n",
        "error_num = (circuit_result_tensor != target[train_num:input_data_num]).sum()\n",
        "\n",
        "print(\"error_num :\",error_num)\n",
        "print(\"error_rate:\",float(error_num)/(input_data_num - train_num))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error_num : tensor(25)\n",
            "error_rate: 0.3125\n"
          ]
        }
      ],
      "metadata": {}
    }
  ]
}